
<!DOCTYPE html>
<html>
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <style>
        body {
            font-family: -apple-system, BlinkMacSystemFont, "Segoe UI", Roboto, "Helvetica Neue", Arial, sans-serif;
            line-height: 1.6;
            color: #333;
            max-width: 900px;
            margin: 0 auto;
            padding: 20px;
            background: #f5f7fa;
        }
        .container {
            background: white;
            border-radius: 16px;
            padding: 40px;
            box-shadow: 0 4px 20px rgba(0,0,0,0.08);
        }
        .header {
            text-align: center;
            padding: 30px 0;
            border-bottom: 3px solid #667eea;
            margin-bottom: 30px;
        }
        .header h1 {
            color: #2d3748;
            margin: 0;
            font-size: 32px;
            background: linear-gradient(135deg, #667eea 0%, #764ba2 100%);
            -webkit-background-clip: text;
            -webkit-text-fill-color: transparent;
        }
        .date {
            color: #718096;
            font-size: 16px;
            margin-top: 10px;
        }
        .briefing-box {
            background: linear-gradient(135deg, #e6fffa 0%, #b2f5ea 100%);
            border-radius: 12px;
            padding: 25px;
            margin-bottom: 40px;
            border-left: 5px solid #38b2ac;
        }
        .briefing-title {
            font-size: 20px;
            font-weight: bold;
            color: #234e52;
            margin-bottom: 15px;
            display: flex;
            align-items: center;
        }
        .section {
            margin: 40px 0;
        }
        .section-header {
            display: flex;
            align-items: center;
            margin-bottom: 25px;
            padding-bottom: 15px;
            border-bottom: 2px solid #e2e8f0;
        }
        .section-icon {
            font-size: 28px;
            margin-right: 15px;
        }
        .section-title {
            font-size: 24px;
            color: #2d3748;
            margin: 0;
        }
        .section-subtitle {
            font-size: 14px;
            color: #718096;
            margin-left: auto;
        }
        /* Paper Cards */
        .paper-card {
            background: #f7fafc;
            border-radius: 12px;
            padding: 20px;
            margin: 15px 0;
            border-left: 4px solid #667eea;
            transition: transform 0.2s, box-shadow 0.2s;
        }
        .paper-card:hover {
            transform: translateX(5px);
            box-shadow: 0 4px 12px rgba(102, 126, 234, 0.15);
        }
        .paper-card.ad { border-left-color: #48bb78; }
        .paper-title { font-size: 16px; font-weight: 600; margin-bottom: 8px; }
        .paper-title a { color: #2d3748; text-decoration: none; }
        .paper-title a:hover { color: #667eea; }
        .paper-authors { font-size: 13px; color: #718096; margin-bottom: 10px; }
        .paper-summary { font-size: 14px; color: #4a5568; line-height: 1.7; }
        .paper-meta { display: flex; gap: 15px; margin-top: 12px; font-size: 12px; }
        .paper-tag { display: inline-block; padding: 3px 10px; background: #edf2f7; border-radius: 12px; color: #4a5568; }
        
        /* Feed Cards */
        .feed-list { list-style: none; padding: 0; }
        .feed-item {
            padding: 15px;
            border-bottom: 1px solid #edf2f7;
            display: flex;
            flex-direction: column;
        }
        .feed-item:last-child { border-bottom: none; }
        .feed-source { 
            font-size: 12px; 
            text-transform: uppercase; 
            color: #718096; 
            font-weight: bold;
            margin-bottom: 4px;
        }
        .feed-title { font-size: 16px; font-weight: 600; margin-bottom: 5px; }
        .feed-title a { color: #2b6cb0; text-decoration: none; }
        .feed-title a:hover { text-decoration: underline; }
        .feed-date { font-size: 12px; color: #a0aec0; }

        .video-card {
            background: linear-gradient(135deg, #1a1a2e 0%, #16213e 100%);
            border-radius: 12px;
            padding: 20px;
            margin: 15px 0;
            color: white;
        }
        .video-title a { color: #ff6b6b; text-decoration: none; }
        
        .footer {
            margin-top: 50px;
            padding-top: 30px;
            border-top: 2px solid #e2e8f0;
            text-align: center;
            color: #718096;
            font-size: 14px;
        }
        .stats-box {
            background: #f1f5f9;
            border-radius: 8px;
            padding: 15px;
            margin-top: 30px;
            border: 1px solid #e2e8f0;
            text-align: left;
        }
    </style>
</head>
<body>
    <div class="container">
        <div class="header">
            <h1>ğŸ”¬ AI ç ”ç©¶å‘¨æŠ¥</h1>
            <div class="date">2026å¹´02æœˆ13æ—¥</div>
        </div>
        
        <!-- AI Daily Briefing -->
        <div class="briefing-box">
            <div class="briefing-title">â˜•ï¸ ä»Šæ—¥ AI ç®€æŠ¥</div>
            <div style="color: #2c7a7b; font-size: 15px; line-height: 1.8;">
                <p>æ— æ³•ç”Ÿæˆä»Šæ—¥ç®€æŠ¥ï¼Œè¯·ç›´æ¥é˜…è¯»ä¸‹æ–¹è¯¦ç»†å†…å®¹ã€‚</p>
            </div>
        </div>
        
        <!-- arXiv Papers -->
        <div class="section">
            <div class="section-header">
                <span class="section-icon">ğŸ“š</span>
                <h2 class="section-title">æ ¸å¿ƒè®ºæ–‡ (ArXiv)</h2>
            </div>
            <h3 style="color: #4a5568; margin-top:20px;">ğŸ”¥ å¤§æ¨¡å‹å‰æ²¿</h3>
            
            <div class="paper-card">
                <div class="paper-title">
                    <a href="https://arxiv.org/abs/2602.12276v1" target="_blank">Agentic Test-Time Scaling for WebAgents</a>
                </div>
                <div class="paper-authors">ğŸ‘¥ Nicholas Lee, Lutfi Eren Erdogan, Chris Joseph John</div>
                <div class="paper-summary">Test-time scaling has become a standard way to improve performance and boost reliability of neural network models. However, its behavior on agentic, multi-step tasks remains less well-understood: small per-step errors can compound over long horizons; and we find that naive policies that uniformly increase sampling show diminishing returns. In this work, we present CATTS, a simple technique for dynamically allocating compute for multi-step agents. We first conduct an empirical study of inference-time scaling for web agents. We find that uniformly increasing per-step compute quickly saturates in long-horizon environments. We then investigate stronger aggregation strategies, including an LLM-based Arbiter that can outperform naive voting, but that can overrule high-consensus decisions. We show that uncertainty statistics derived from the agent's own vote distribution (entropy and top-1/top-2 margin) correlate with downstream success and provide a practical signal for dynamic compute allocation. Based on these findings, we introduce Confidence-Aware Test-Time Scaling (CATTS), which uses vote-derived uncertainty to allocate compute only when decisions are genuinely contentious. CATTS improves performance on WebArena-Lite and GoBrowse by up to 9.1% over React while using up to 2.3x fewer tokens than uniform scaling, providing both efficiency gains and an interpretable decision rule.</div>
                <div class="paper-meta">
                    <span class="paper-tag">ğŸ“… 2026-02-12</span>
                    <a href="https://arxiv.org/pdf/2602.12276v1" class="paper-link" target="_blank">ğŸ“„ PDF</a>
                </div>
            </div>
            

            <div class="paper-card">
                <div class="paper-title">
                    <a href="https://arxiv.org/abs/2602.12262v1" target="_blank">T3D: Few-Step Diffusion Language Models via Trajectory Self-Distillation with Direct Discriminative Optimization</a>
                </div>
                <div class="paper-authors">ğŸ‘¥ Tunyu Zhang, Xinxi Zhang, Ligong Han</div>
                <div class="paper-summary">Diffusion large language models (DLLMs) have the potential to enable fast text generation by decoding multiple tokens in parallel. However, in practice, their inference efficiency is constrained by the need for many refinement steps, while aggressively reducing the number of steps leads to a substantial degradation in generation quality. To alleviate this, we propose a trajectory self-distillation framework that improves few-step decoding by distilling the model's own generative trajectories. We incorporate Direct Discriminative Optimization (DDO), a reverse-KL objective that promotes mode-seeking distillation and encourages the student to concentrate on high-probability teacher modes. Across benchmarks, our approach consistently outperforms strong few-step baselines and standard training under tight step budgets. Although full-step decoding remains superior, we substantially narrow the gap, establishing a strong foundation towards practical few-step DLLMs. The source code is available at https://github.com/Tyrion58/T3D.</div>
                <div class="paper-meta">
                    <span class="paper-tag">ğŸ“… 2026-02-12</span>
                    <a href="https://arxiv.org/pdf/2602.12262v1" class="paper-link" target="_blank">ğŸ“„ PDF</a>
                </div>
            </div>
            

            <div class="paper-card">
                <div class="paper-title">
                    <a href="https://arxiv.org/abs/2602.12251v1" target="_blank">A technical curriculum on language-oriented artificial intelligence in translation and specialised communication</a>
                </div>
                <div class="paper-authors">ğŸ‘¥ Ralph KrÃ¼ger</div>
                <div class="paper-summary">This paper presents a technical curriculum on language-oriented artificial intelligence (AI) in the language and translation (L&T) industry. The curriculum aims to foster domain-specific technical AI literacy among stakeholders in the fields of translation and specialised communication by exposing them to the conceptual and technical/algorithmic foundations of modern language-oriented AI in an accessible way. The core curriculum focuses on 1) vector embeddings, 2) the technical foundations of neural networks, 3) tokenization and 4) transformer neural networks. It is intended to help users develop computational thinking as well as algorithmic awareness and algorithmic agency, ultimately contributing to their digital resilience in AI-driven work environments. The didactic suitability of the curriculum was tested in an AI-focused MA course at the Institute of Translation and Multilingual Communication at TH Koeln. Results suggest the didactic effectiveness of the curriculum, but participant feedback indicates that it should be embedded into higher-level didactic scaffolding - e.g., in the form of lecturer support - in order to enable optimal learning conditions.</div>
                <div class="paper-meta">
                    <span class="paper-tag">ğŸ“… 2026-02-12</span>
                    <a href="https://arxiv.org/pdf/2602.12251v1" class="paper-link" target="_blank">ğŸ“„ PDF</a>
                </div>
            </div>
            

            <div class="paper-card">
                <div class="paper-title">
                    <a href="https://arxiv.org/abs/2602.12241v1" target="_blank">Moonshine v2: Ergodic Streaming Encoder ASR for Latency-Critical Speech Applications</a>
                </div>
                <div class="paper-authors">ğŸ‘¥ Manjunath Kudlur, Evan King, James Wang</div>
                <div class="paper-summary">Latency-critical speech applications (e.g., live transcription, voice commands, and real-time translation) demand low time-to-first-token (TTFT) and high transcription accuracy, particularly on resource-constrained edge devices. Full-attention Transformer encoders remain a strong accuracy baseline for automatic speech recognition (ASR) because every frame can directly attend to every other frame, which resolves otherwise locally ambiguous acoustics using distant lexical context. However, this global dependency incurs quadratic complexity in sequence length, inducing an inherent "encode-the-whole-utterance" latency profile. For streaming use cases, this causes TTFT to grow linearly with utterance length as the encoder must process the entire prefix before any decoder token can be emitted. To better meet the needs of on-device, streaming ASR use cases we introduce Moonshine v2, an ergodic streaming-encoder ASR model that employs sliding-window self-attention to achieve bounded, low-latency inference while preserving strong local context. Our models achieve state of the art word error rates across standard benchmarks, attaining accuracy on-par with models 6x their size while running significantly faster. These results demonstrate that carefully designed local attention is competitive with the accuracy of full attention at a fraction of the size and latency cost, opening new possibilities for interactive speech interfaces on edge devices.</div>
                <div class="paper-meta">
                    <span class="paper-tag">ğŸ“… 2026-02-12</span>
                    <a href="https://arxiv.org/pdf/2602.12241v1" class="paper-link" target="_blank">ğŸ“„ PDF</a>
                </div>
            </div>
            

            <div class="paper-card">
                <div class="paper-title">
                    <a href="https://arxiv.org/abs/2602.12235v1" target="_blank">Detecting Overflow in Compressed Token Representations for Retrieval-Augmented Generation</a>
                </div>
                <div class="paper-authors">ğŸ‘¥ Julia Belikova, Danila Rozhevskii, Dennis Svirin</div>
                <div class="paper-summary">Efficient long-context processing remains a crucial challenge for contemporary large language models (LLMs), especially in resource-constrained environments. Soft compression architectures promise to extend effective context length by replacing long token sequences with smaller sets of learned compressed tokens. Yet, the limits of compressibility -- and when compression begins to erase task-relevant content -- remain underexplored. In this paper, we define \emph{token overflow} as a regime in which compressed representations no longer contain sufficient information to answer a given query, and propose a methodology to characterize and detect it. In the xRAG soft-compression setting, we find that query-agnostic saturation statistics reliably separate compressed from uncompressed token representations, providing a practical tool for identifying compressed tokens but showing limited overflow detection capability. Lightweight probing classifiers over both query and context xRAG representations detect overflow with 0.72 AUC-ROC on average on HotpotQA, SQuADv2, and TriviaQA datasets, demonstrating that incorporating query information improves detection performance. These results advance from query-independent diagnostics to query-aware detectors, enabling low-cost pre-LLM gating to mitigate compression-induced errors.</div>
                <div class="paper-meta">
                    <span class="paper-tag">ğŸ“… 2026-02-12</span>
                    <a href="https://arxiv.org/pdf/2602.12235v1" class="paper-link" target="_blank">ğŸ“„ PDF</a>
                </div>
            </div>
            

            <div class="paper-card">
                <div class="paper-title">
                    <a href="https://arxiv.org/abs/2602.12196v1" target="_blank">Visual Reasoning Benchmark: Evaluating Multimodal LLMs on Classroom-Authentic Visual Problems from Primary Education</a>
                </div>
                <div class="paper-authors">ğŸ‘¥ Mohamed Huti, Alasdair Mackintosh, Amy Waldock</div>
                <div class="paper-summary">AI models have achieved state-of-the-art results in textual reasoning; however, their ability to reason over spatial and relational structures remains a critical bottleneck -- particularly in early-grade maths, which relies heavily on visuals. This paper introduces the visual reasoning benchmark (VRB), a novel dataset designed to evaluate Multimodal Large Language Models (MLLMs) on their ability to solve authentic visual problems from classrooms. This benchmark is built on a set of 701 questions sourced from primary school examinations in Zambia and India, which cover a range of tasks such as reasoning by analogy, pattern completion, and spatial matching. We outline the methodology and development of the benchmark which intentionally uses unedited, minimal-text images to test if models can meet realistic needs of primary education. Our findings reveal a ``jagged frontier'' of capability where models demonstrate better proficiency in static skills such as counting and scaling, but reach a distinct ``spatial ceiling'' when faced with dynamic operations like folding, reflection, and rotation. These weaknesses pose a risk for classroom use on visual reasoning problems, with the potential for incorrect marking, false scaffolding, and reinforcing student misconceptions. Consequently, education-focused benchmarks like the VRB are essential for determining the functional boundaries of multimodal tools used in classrooms.</div>
                <div class="paper-meta">
                    <span class="paper-tag">ğŸ“… 2026-02-12</span>
                    <a href="https://arxiv.org/pdf/2602.12196v1" class="paper-link" target="_blank">ğŸ“„ PDF</a>
                </div>
            </div>
            
            
            <h3 style="color: #4a5568; margin-top:30px;">ğŸ“Š å¹¿å‘Šä¸æ¨èç®—æ³•</h3>
            
            <div class="paper-card ad">
                <div class="paper-title">
                    <a href="https://arxiv.org/abs/2602.12041v1" target="_blank">Compress, Cross and Scale: Multi-Level Compression Cross Networks for Efficient Scaling in Recommender Systems</a>
                </div>
                <div class="paper-authors">ğŸ‘¥ Heng Yu, Xiangjun Zhou, Jie Xia</div>
                <div class="paper-summary">Modeling high-order feature interactions efficiently is a central challenge in click-through rate and conversion rate prediction. Modern industrial recommender systems are predominantly built upon deep learning recommendation models, where the interaction backbone plays a critical role in determining both predictive performance and system efficiency. However, existing interaction modules often struggle to simultaneously achieve strong interaction capacity, high computational efficiency, and good scalability, resulting in limited ROI when models are scaled under strict production constraints. In this work, we propose MLCC, a structured feature interaction architecture that organizes feature crosses through hierarchical compression and dynamic composition, which can efficiently capture high-order feature dependencies while maintaining favorable computational complexity. We further introduce MC-MLCC, a Multi-Channel extension that decomposes feature interactions into parallel subspaces, enabling efficient horizontal scaling with improved representation capacity and significantly reduced parameter growth. Extensive experiments on three public benchmarks and a large-scale industrial dataset show that our proposed models consistently outperform strong DLRM-style baselines by up to 0.52 AUC, while reducing model parameters and FLOPs by up to 26$\times$ under comparable performance. Comprehensive scaling analyses demonstrate stable and predictable scaling behavior across embedding dimension, head number, and channel count, with channel-based scaling achieving substantially better efficiency than conventional embedding inflation. Finally, online A/B testing on a real-world advertising platform validates the practical effectiveness of our approach, which has been widely adopted in Bilibili advertising system under strict latency and resource constraints.</div>
                <div class="paper-meta">
                    <span class="paper-tag">ğŸ“… 2026-02-12</span>
                    <a href="https://arxiv.org/pdf/2602.12041v1" class="paper-link" target="_blank">ğŸ“„ PDF</a>
                </div>
            </div>
            

            <div class="paper-card ad">
                <div class="paper-title">
                    <a href="https://arxiv.org/abs/2602.10811v1" target="_blank">EST: Towards Efficient Scaling Laws in Click-Through Rate Prediction via Unified Modeling</a>
                </div>
                <div class="paper-authors">ğŸ‘¥ Mingyang Liu, Yong Bai, Zhangming Chan</div>
                <div class="paper-summary">Efficiently scaling industrial Click-Through Rate (CTR) prediction has recently attracted significant research attention. Existing approaches typically employ early aggregation of user behaviors to maintain efficiency. However, such non-unified or partially unified modeling creates an information bottleneck by discarding fine-grained, token-level signals essential for unlocking scaling gains. In this work, we revisit the fundamental distinctions between CTR prediction and Large Language Models (LLMs), identifying two critical properties: the asymmetry in information density between behavioral and non-behavioral features, and the modality-specific priors of content-rich signals. Accordingly, we propose the Efficiently Scalable Transformer (EST), which achieves fully unified modeling by processing all raw inputs in a single sequence without lossy aggregation. EST integrates two modules: Lightweight Cross-Attention (LCA), which prunes redundant self-interactions to focus on high-impact cross-feature dependencies, and Content Sparse Attention (CSA), which utilizes content similarity to dynamically select high-signal behaviors. Extensive experiments show that EST exhibits a stable and efficient power-law scaling relationship, enabling predictable performance gains with model scale. Deployed on Taobao's display advertising platform, EST significantly outperforms production baselines, delivering a 3.27\% RPM (Revenue Per Mile) increase and a 1.22\% CTR lift, establishing a practical pathway for scalable industrial CTR prediction models.</div>
                <div class="paper-meta">
                    <span class="paper-tag">ğŸ“… 2026-02-11</span>
                    <a href="https://arxiv.org/pdf/2602.10811v1" class="paper-link" target="_blank">ğŸ“„ PDF</a>
                </div>
            </div>
            

            <div class="paper-card ad">
                <div class="paper-title">
                    <a href="https://arxiv.org/abs/2602.11410v1" target="_blank">CADET: Context-Conditioned Ads CTR Prediction With a Decoder-Only Transformer</a>
                </div>
                <div class="paper-authors">ğŸ‘¥ David Pardoe, Neil Daftary, Miro Furtado</div>
                <div class="paper-summary">Click-through rate (CTR) prediction is fundamental to online advertising systems. While Deep Learning Recommendation Models (DLRMs) with explicit feature interactions have long dominated this domain, recent advances in generative recommenders have shown promising results in content recommendation. However, adapting these transformer-based architectures to ads CTR prediction still presents unique challenges, including handling post-scoring contextual signals, maintaining offline-online consistency, and scaling to industrial workloads. We present CADET (Context-Conditioned Ads Decoder-Only Transformer), an end-to-end decoder-only transformer for ads CTR prediction deployed at LinkedIn. Our approach introduces several key innovations: (1) a context-conditioned decoding architecture with multi-tower prediction heads that explicitly model post-scoring signals such as ad position, resolving the chicken-and-egg problem between predicted CTR and ranking; (2) a self-gated attention mechanism that stabilizes training by adaptively regulating information flow at both representation and interaction levels; (3) a timestamp-based variant of Rotary Position Embedding (RoPE) that captures temporal relationships across timescales from seconds to months; (4) session masking strategies that prevent the model from learning dependencies on unavailable in-session events, addressing train-serve skew; and (5) production engineering techniques including tensor packing, sequence chunking, and custom Flash Attention kernels that enable efficient training and serving at scale. In online A/B testing, CADET achieves a 11.04\% CTR lift compared to the production LiRank baseline model, a hybrid ensemble of DCNv2 and sequential encoders. The system has been successfully deployed on LinkedIn's advertising platform, serving the main traffic for homefeed sponsored updates.</div>
                <div class="paper-meta">
                    <span class="paper-tag">ğŸ“… 2026-02-11</span>
                    <a href="https://arxiv.org/pdf/2602.11410v1" class="paper-link" target="_blank">ğŸ“„ PDF</a>
                </div>
            </div>
            

            <div class="paper-card ad">
                <div class="paper-title">
                    <a href="https://arxiv.org/abs/2602.09194v1" target="_blank">ML-DCN: Masked Low-Rank Deep Crossing Network Towards Scalable Ads Click-through Rate Prediction at Pinterest</a>
                </div>
                <div class="paper-authors">ğŸ‘¥ Jiacheng Li, Yixiong Meng, Yi wu</div>
                <div class="paper-summary">Deep learning recommendation systems rely on feature interaction modules to model complex user-item relationships across sparse categorical and dense features. In large-scale ad ranking, increasing model capacity is a promising path to improving both predictive performance and business outcomes, yet production serving budgets impose strict constraints on latency and FLOPs. This creates a central tension: we want interaction modules that both scale effectively with additional compute and remain compute-efficient at serving time. In this work, we study how to scale feature interaction modules under a fixed serving budget. We find that naively scaling DCNv2 and MaskNet, despite their widespread adoption in industry, yields rapidly diminishing offline gains in the Pinterest ads ranking system. To overcome aforementioned limitations, we propose ML-DCN, an interaction module that integrates an instance-conditioned mask into a low-rank crossing layer, enabling per-example selection and amplification of salient interaction directions while maintaining efficient computation. This novel architecture combines the strengths of DCNv2 and MaskNet, scales efficiently with increased compute, and achieves state-of-the-art performance. Experiments on a large internal Pinterest ads dataset show that ML-DCN achieves higher AUC than DCNv2, MaskNet, and recent scaling-oriented alternatives at matched FLOPs, and it scales more favorably overall as compute increases, exhibiting a stronger AUC-FLOPs trade-off. Finally, online A/B tests demonstrate statistically significant improvements in key ads metrics (including CTR and click-quality measures) and ML-DCN has been deployed in the production system with neutral serving cost.</div>
                <div class="paper-meta">
                    <span class="paper-tag">ğŸ“… 2026-02-09</span>
                    <a href="https://arxiv.org/pdf/2602.09194v1" class="paper-link" target="_blank">ğŸ“„ PDF</a>
                </div>
            </div>
            
        </div>
        
        <!-- RSS Feeds -->
        <div class="section">
            <div class="section-header">
                <span class="section-icon">ğŸ“¡</span>
                <h2 class="section-title">ä¸šç•ŒåŠ¨æ€</h2>
            </div>
            
            <div style="display: grid; grid-template-columns: 1fr 1fr; gap: 40px;">
                <div>
                    <h3 style="border-bottom: 2px solid #ed8936; padding-bottom: 10px; color: #c05621;">ğŸ¢ AI Labs æ›´æ–°</h3>
                    <div class="feed-list">
            <div class="feed-item">
                <div class="feed-source">Google The Keyword</div>
                <div class="feed-title"><a href="https://blog.google/products-and-platforms/devices/pixel/lunar-new-year-2026-translation-tools/" target="_blank">3 ways you can use Pixel for translation help this Lunar Year.</a></div>
                <div class="feed-date">02-12</div>
            </div>
            
            <div class="feed-item">
                <div class="feed-source">Google The Keyword</div>
                <div class="feed-title"><a href="https://blog.google/innovation-and-ai/models-and-research/gemini-models/gemini-3-deep-think/" target="_blank">Gemini 3 Deep Think: Advancing science, research and engineering</a></div>
                <div class="feed-date">02-12</div>
            </div>
            
            <div class="feed-item">
                <div class="feed-source">Google The Keyword</div>
                <div class="feed-title"><a href="https://blog.google/innovation-and-ai/infrastructure-and-cloud/google-cloud/gtig-report-ai-cyber-attacks-feb-2026/" target="_blank">Our new report details the latest ways threat actors are misusing AI.</a></div>
                <div class="feed-date">02-12</div>
            </div>
            
            <div class="feed-item">
                <div class="feed-source">OpenAI</div>
                <div class="feed-title"><a href="https://openai.com/index/introducing-gpt-5-3-codex-spark" target="_blank">Introducing GPT-5.3-Codex-Spark</a></div>
                <div class="feed-date">02-12</div>
            </div>
            
            <div class="feed-item">
                <div class="feed-source">Google The Keyword</div>
                <div class="feed-title"><a href="https://blog.google/company-news/outreach-and-initiatives/arts-culture/india-ai-creativity/" target="_blank">3 new experiences bridging AI with Indiaâ€™s timeless heritage</a></div>
                <div class="feed-date">02-12</div>
            </div>
            
            <div class="feed-item">
                <div class="feed-source">Google The Keyword</div>
                <div class="feed-title"><a href="https://blog.google/innovation-and-ai/technology/safety-security/how-were-helping-democracies-stay-ahead-of-digital-threats/" target="_blank">How weâ€™re helping democracies stay ahead of digital threats</a></div>
                <div class="feed-date">02-11</div>
            </div>
            
            <div class="feed-item">
                <div class="feed-source">Google The Keyword</div>
                <div class="feed-title"><a href="https://blog.google/products/ads-commerce/is-your-campaign-structure-holding-you-back-in-the-era-of-ai/" target="_blank">Is your campaign structure holding you back in the era of AI?</a></div>
                <div class="feed-date">02-11</div>
            </div>
            
            <div class="feed-item">
                <div class="feed-source">Meta Engineering</div>
                <div class="feed-title"><a href="https://engineering.fb.com/2026/02/11/developer-tools/the-death-of-traditional-testing-agentic-development-jit-testing-revival/" target="_blank">The Death of Traditional Testing: Agentic Development Broke a 50-Year-Old Field, JiTTesting Can Revive It</a></div>
                <div class="feed-date">02-11</div>
            </div>
            </div>
                </div>
                <div>
                    <h3 style="border-bottom: 2px solid #48bb78; padding-bottom: 10px; color: #2f855a;">ğŸ’° é¡¶çº§é£æŠ•è§‚ç‚¹</h3>
                    <div class="feed-list">
            <div class="feed-item">
                <div class="feed-source">Sequoia Capital</div>
                <div class="feed-title"><a href="https://sequoiacap.com/article/the-opening-midgame-and-endgame-in-startups/" target="_blank">The Opening, Midgame and Endgame in Startups</a></div>
                <div class="feed-date">02-11</div>
            </div>
            </div>
                </div>
            </div>
            
            <div style="margin-top: 40px;">
                 <h3 style="border-bottom: 2px solid #4299e1; padding-bottom: 10px; color: #2b6cb0;">ğŸ“° ç§‘æŠ€æ–°é—»ç²¾é€‰</h3>
                 <div class="feed-list">
            <div class="feed-item">
                <div class="feed-source">Ars Technica</div>
                <div class="feed-title"><a href="https://arstechnica.com/space/2026/02/when-amazon-badly-needed-a-ride-europes-ariane-6-rocket-delivered/" target="_blank">When Amazon badly needed a ride, Europe's Ariane 6 rocket delivered</a></div>
                <div class="feed-date">02-13</div>
            </div>
            
            <div class="feed-item">
                <div class="feed-source">TechCrunch</div>
                <div class="feed-title"><a href="https://techcrunch.com/2026/02/12/for-1-million-you-can-pay-bryan-johnson-or-bryanai-to-teach-you-how-to-live-longer/" target="_blank">For $1M, you can pay Bryan Johnson (or BryanAI?) to teach you how to live longer</a></div>
                <div class="feed-date">02-12</div>
            </div>
            
            <div class="feed-item">
                <div class="feed-source">The Verge</div>
                <div class="feed-title"><a href="https://www.theverge.com/games/878375/god-of-war-sons-of-sparta-trilogy-sony-playstation-ps5-release-date-trailer" target="_blank">A surprise God of War prequel is out on the PS5 right now</a></div>
                <div class="feed-date">02-12</div>
            </div>
            
            <div class="feed-item">
                <div class="feed-source">TechCrunch</div>
                <div class="feed-title"><a href="https://techcrunch.com/2026/02/12/amid-disappointing-earnings-pinterest-claims-it-sees-more-searches-than-chatgpt/" target="_blank">Amid disappointing earnings, Pinterest claims it sees more searches than ChatGPT</a></div>
                <div class="feed-date">02-12</div>
            </div>
            
            <div class="feed-item">
                <div class="feed-source">TechCrunch AI</div>
                <div class="feed-title"><a href="https://techcrunch.com/2026/02/12/amid-disappointing-earnings-pinterest-claims-it-sees-more-searches-than-chatgpt/" target="_blank">Amid disappointing earnings, Pinterest claims it sees more searches than ChatGPT</a></div>
                <div class="feed-date">02-12</div>
            </div>
            
            <div class="feed-item">
                <div class="feed-source">TechCrunch</div>
                <div class="feed-title"><a href="https://techcrunch.com/2026/02/12/ibm-will-hire-your-entry-level-talent-in-the-age-of-ai/" target="_blank">IBMÂ will hire your entry-level talent in the age of AI</a></div>
                <div class="feed-date">02-12</div>
            </div>
            
            <div class="feed-item">
                <div class="feed-source">TechCrunch AI</div>
                <div class="feed-title"><a href="https://techcrunch.com/2026/02/12/ibm-will-hire-your-entry-level-talent-in-the-age-of-ai/" target="_blank">IBMÂ will hire your entry-level talent in the age of AI</a></div>
                <div class="feed-date">02-12</div>
            </div>
            
            <div class="feed-item">
                <div class="feed-source">TechCrunch</div>
                <div class="feed-title"><a href="https://techcrunch.com/2026/02/12/rivian-was-saved-by-software-in-2025/" target="_blank">Rivian was saved by software in 2025</a></div>
                <div class="feed-date">02-12</div>
            </div>
            </div>
            </div>
        </div>
        
        <!-- YouTube Videos -->
        <div class="section">
            <div class="section-header">
                <span class="section-icon">ğŸ¬</span>
                <h2 class="section-title">ç§‘æŠ€é¢†è¢–è®¿è°ˆ</h2>
            </div>
            <h4 style="margin: 20px 0 10px 0; color: #553c9a;">ğŸ‘¤ Elon Musk</h4>

                <div class="video-card">
                    <div class="video-title"><a href="https://www.youtube.com/watch?v=IgifEgm1-e0" target="_blank">ğŸ¥ Conversation with Elon Musk | World Economic Forum Annual Meeting 2026</a></div>
                    <div style="font-size: 12px; color: #a0aec0; margin-top:5px;">Please join us for a conversation between Elon Musk, CEO of Tesla; Chief Engineer, SpaceX; CTO, xAI;...</div>
                </div>
                

                <div class="video-card">
                    <div class="video-title"><a href="https://www.youtube.com/watch?v=UrB2tQDVLLo" target="_blank">ğŸ¥ Tesla CEO Elon Musk speaks at the World Economic Forum in Davos, Switzerland â€” 1/22/2026</a></div>
                    <div style="font-size: 12px; color: #a0aec0; margin-top:5px;">Elon Musk, CEO of Tesla; chief engineer of SpaceX; and CTO of xAI joins Laurence D. Fink, chair and ...</div>
                </div>
                
<h4 style="margin: 20px 0 10px 0; color: #553c9a;">ğŸ‘¤ Jensen Huang</h4>

                <div class="video-card">
                    <div class="video-title"><a href="https://www.youtube.com/watch?v=0NBILspM4c4" target="_blank">ğŸ¥ NVIDIA Live with CEO Jensen Huang</a></div>
                    <div style="font-size: 12px; color: #a0aec0; margin-top:5px;">Live from CES in Las Vegas, NVIDIA CEO Jensen Huang shares how the next generation of accelerated co...</div>
                </div>
                

                <div class="video-card">
                    <div class="video-title"><a href="https://www.youtube.com/watch?v=vd7KdtB90DQ" target="_blank">ğŸ¥ NVIDIA 2026: The Architecture of the Future (Full Keynote by Jensen Huang)</a></div>
                    <div style="font-size: 12px; color: #a0aec0; margin-top:5px;">"The AI Revolution has reached its next phase. Join NVIDIA CEO Jensen Huang as he reveals the ground...</div>
                </div>
                
<h4 style="margin: 20px 0 10px 0; color: #553c9a;">ğŸ‘¤ Sam Altman</h4>

                <div class="video-card">
                    <div class="video-title"><a href="https://www.youtube.com/watch?v=Q_AWj6uq3UU" target="_blank">ğŸ¥ SAM ALTMAN: The Future of AI Agents and the End of Search (Full Interview 2026)</a></div>
                    <div style="font-size: 12px; color: #a0aec0; margin-top:5px;">"The AI revolution is accelerating. In this exclusive TED session, OpenAI CEO Sam Altman discusses t...</div>
                </div>
                

                <div class="video-card">
                    <div class="video-title"><a href="https://www.youtube.com/watch?v=qtvZwc7VXBk" target="_blank">ğŸ¥ Sam Altman: Das wird dir niemand so sagenâ€¦</a></div>
                    <div style="font-size: 12px; color: #a0aec0; margin-top:5px;">SamAltman #OpenAI #KI Sam Altman wirkt wie das â€perfekteâ€œ Gesicht der KI-Revolution: ruhig, smart, v...</div>
                </div>
                
        </div>
        
        <!-- New Sources: GitHub, Reddit, HN -->
        
            <div class="section">
                <div class="section-header">
                    <span class="section-icon">ğŸŒ</span>
                    <h2 class="section-title">ç¤¾åŒºç²¾é€‰ (AI Curated)</h2>
                </div>
                
            <div style="margin-bottom: 30px;">
                <h3 style="border-bottom: 2px solid #f59e0b; padding-bottom: 10px; color: #d97706;">ğŸ”¥ GitHub è¶‹åŠ¿é¡¹ç›®</h3>
                
                <div style="padding: 10px; border-left: 3px solid #cbd5e0; margin: 8px 0;">
                    <a href="https://github.com/PeonPing/peon-ping" target="_blank" style="color: #2b6cb0;">
                        PeonPing/peon-ping
                    </a>
                </div>
                

                <div style="padding: 10px; border-left: 3px solid #cbd5e0; margin: 8px 0;">
                    <a href="https://github.com/xyzeva/k-id-age-verifier" target="_blank" style="color: #2b6cb0;">
                        xyzeva/k-id-age-verifier
                    </a>
                </div>
                

                <div style="padding: 10px; border-left: 3px solid #cbd5e0; margin: 8px 0;">
                    <a href="https://github.com/promptpirate-x/discord-id-bypass-tool" target="_blank" style="color: #2b6cb0;">
                        promptpirate-x/discord-id-bypass-tool
                    </a>
                </div>
                
            </div>
            
            <div style="margin-bottom: 30px;">
                <h3 style="border-bottom: 2px solid #f97316; padding-bottom: 10px; color: #ea580c;">ğŸŸ  Hacker News ç²¾é€‰</h3>
                
                <div style="padding: 10px; border-left: 3px solid #cbd5e0; margin: 8px 0;">
                    <a href="https://news.ycombinator.com/item?id=46992553" target="_blank" style="color: #2b6cb0;">
                        GPTâ€‘5.3â€‘Codexâ€‘Spark
                    </a>
                </div>
                

                <div style="padding: 10px; border-left: 3px solid #cbd5e0; margin: 8px 0;">
                    <a href="https://news.ycombinator.com/item?id=46991240" target="_blank" style="color: #2b6cb0;">
                        Gemini 3 Deep Think
                    </a>
                </div>
                

                <div style="padding: 10px; border-left: 3px solid #cbd5e0; margin: 8px 0;">
                    <a href="https://news.ycombinator.com/item?id=46990729" target="_blank" style="color: #2b6cb0;">
                        An AI agent published a hit piece on me
                    </a>
                </div>
                
            </div>
            
            </div>
            
        
        
        <div class="stats-box">
            <h4 style="margin:0 0 10px 0; color:#4a5568;">âš™ï¸ ç³»ç»Ÿè¿è¡Œç»Ÿè®¡</h4>
            <div style="display:flex; justify-content:space-between; font-size:12px; color:#718096;">
                <div>
                    <strong>ğŸ¤– AI æ¨¡å‹ (Qwen-72B)</strong><br>
                    è°ƒç”¨æ¬¡æ•°: 0<br>
                    Input Tokens: 0<br>
                    Output Tokens: 0
                </div>
                <div>
                    <strong>ğŸ¬ YouTube API</strong><br>
                    API è°ƒç”¨: 17<br>
                    Quota æ¶ˆè€—: 908 units
                </div>
            </div>
        </div>
        
        
        <div class="footer">
            <p>ğŸ“… 2026å¹´02æœˆ13æ—¥ | Daily Info System</p>
            <p>ğŸ’¡ Stay Hungry, Stay Foolish</p>
        </div>
    </div>
</body>
</html>
