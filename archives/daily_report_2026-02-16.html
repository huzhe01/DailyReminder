
<!DOCTYPE html>
<html>
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <style>
        body {
            font-family: -apple-system, BlinkMacSystemFont, "Segoe UI", Roboto, "Helvetica Neue", Arial, sans-serif;
            line-height: 1.6;
            color: #333;
            max-width: 900px;
            margin: 0 auto;
            padding: 20px;
            background: #f5f7fa;
        }
        .container {
            background: white;
            border-radius: 16px;
            padding: 40px;
            box-shadow: 0 4px 20px rgba(0,0,0,0.08);
        }
        .header {
            text-align: center;
            padding: 30px 0;
            border-bottom: 3px solid #667eea;
            margin-bottom: 30px;
        }
        .header h1 {
            color: #2d3748;
            margin: 0;
            font-size: 32px;
            background: linear-gradient(135deg, #667eea 0%, #764ba2 100%);
            -webkit-background-clip: text;
            -webkit-text-fill-color: transparent;
        }
        .date {
            color: #718096;
            font-size: 16px;
            margin-top: 10px;
        }
        .briefing-box {
            background: linear-gradient(135deg, #e6fffa 0%, #b2f5ea 100%);
            border-radius: 12px;
            padding: 25px;
            margin-bottom: 40px;
            border-left: 5px solid #38b2ac;
        }
        .briefing-title {
            font-size: 20px;
            font-weight: bold;
            color: #234e52;
            margin-bottom: 15px;
            display: flex;
            align-items: center;
        }
        .section {
            margin: 40px 0;
        }
        .section-header {
            display: flex;
            align-items: center;
            margin-bottom: 25px;
            padding-bottom: 15px;
            border-bottom: 2px solid #e2e8f0;
        }
        .section-icon {
            font-size: 28px;
            margin-right: 15px;
        }
        .section-title {
            font-size: 24px;
            color: #2d3748;
            margin: 0;
        }
        .section-subtitle {
            font-size: 14px;
            color: #718096;
            margin-left: auto;
        }
        /* Paper Cards */
        .paper-card {
            background: #f7fafc;
            border-radius: 12px;
            padding: 20px;
            margin: 15px 0;
            border-left: 4px solid #667eea;
            transition: transform 0.2s, box-shadow 0.2s;
        }
        .paper-card:hover {
            transform: translateX(5px);
            box-shadow: 0 4px 12px rgba(102, 126, 234, 0.15);
        }
        .paper-card.ad { border-left-color: #48bb78; }
        .paper-title { font-size: 16px; font-weight: 600; margin-bottom: 8px; }
        .paper-title a { color: #2d3748; text-decoration: none; }
        .paper-title a:hover { color: #667eea; }
        .paper-authors { font-size: 13px; color: #718096; margin-bottom: 10px; }
        .paper-summary { font-size: 14px; color: #4a5568; line-height: 1.7; }
        .paper-meta { display: flex; gap: 15px; margin-top: 12px; font-size: 12px; }
        .paper-tag { display: inline-block; padding: 3px 10px; background: #edf2f7; border-radius: 12px; color: #4a5568; }
        
        /* Feed Cards */
        .feed-list { list-style: none; padding: 0; }
        .feed-item {
            padding: 15px;
            border-bottom: 1px solid #edf2f7;
            display: flex;
            flex-direction: column;
        }
        .feed-item:last-child { border-bottom: none; }
        .feed-source { 
            font-size: 12px; 
            text-transform: uppercase; 
            color: #718096; 
            font-weight: bold;
            margin-bottom: 4px;
        }
        .feed-title { font-size: 16px; font-weight: 600; margin-bottom: 5px; }
        .feed-title a { color: #2b6cb0; text-decoration: none; }
        .feed-title a:hover { text-decoration: underline; }
        .feed-date { font-size: 12px; color: #a0aec0; }

        .video-card {
            background: linear-gradient(135deg, #1a1a2e 0%, #16213e 100%);
            border-radius: 12px;
            padding: 20px;
            margin: 15px 0;
            color: white;
        }
        .video-title a { color: #ff6b6b; text-decoration: none; }
        
        .footer {
            margin-top: 50px;
            padding-top: 30px;
            border-top: 2px solid #e2e8f0;
            text-align: center;
            color: #718096;
            font-size: 14px;
        }
        .stats-box {
            background: #f1f5f9;
            border-radius: 8px;
            padding: 15px;
            margin-top: 30px;
            border: 1px solid #e2e8f0;
            text-align: left;
        }
    </style>
</head>
<body>
    <div class="container">
        <div class="header">
            <h1>ğŸ”¬ AI ç ”ç©¶å‘¨æŠ¥</h1>
            <div class="date">2026å¹´02æœˆ16æ—¥</div>
        </div>
        
        <!-- AI Daily Briefing -->
        <div class="briefing-box">
            <div class="briefing-title">â˜•ï¸ ä»Šæ—¥ AI ç®€æŠ¥</div>
            <div style="color: #2c7a7b; font-size: 15px; line-height: 1.8;">
                <p>æ— æ³•ç”Ÿæˆä»Šæ—¥ç®€æŠ¥ï¼Œè¯·ç›´æ¥é˜…è¯»ä¸‹æ–¹è¯¦ç»†å†…å®¹ã€‚</p>
            </div>
        </div>
        
        <!-- arXiv Papers -->
        <div class="section">
            <div class="section-header">
                <span class="section-icon">ğŸ“š</span>
                <h2 class="section-title">æ ¸å¿ƒè®ºæ–‡ (ArXiv)</h2>
            </div>
            <h3 style="color: #4a5568; margin-top:20px;">ğŸ”¥ å¤§æ¨¡å‹å‰æ²¿</h3>
            
            <div class="paper-card">
                <div class="paper-title">
                    <a href="https://arxiv.org/abs/2602.13194v1" target="_blank">Semantic Chunking and the Entropy of Natural Language</a>
                </div>
                <div class="paper-authors">ğŸ‘¥ Weishun Zhong, Doron Sivan, Tankut Can</div>
                <div class="paper-summary">The entropy rate of printed English is famously estimated to be about one bit per character, a benchmark that modern large language models (LLMs) have only recently approached. This entropy rate implies that English contains nearly 80 percent redundancy relative to the five bits per character expected for random text. We introduce a statistical model that attempts to capture the intricate multi-scale structure of natural language, providing a first-principles account of this redundancy level. Our model describes a procedure of self-similarly segmenting text into semantically coherent chunks down to the single-word level. The semantic structure of the text can then be hierarchically decomposed, allowing for analytical treatment. Numerical experiments with modern LLMs and open datasets suggest that our model quantitatively captures the structure of real texts at different levels of the semantic hierarchy. The entropy rate predicted by our model agrees with the estimated entropy rate of printed English. Moreover, our theory further reveals that the entropy rate of natural language is not fixed but should increase systematically with the semantic complexity of corpora, which are captured by the only free parameter in our model.</div>
                <div class="paper-meta">
                    <span class="paper-tag">ğŸ“… 2026-02-13</span>
                    <a href="https://arxiv.org/pdf/2602.13194v1" class="paper-link" target="_blank">ğŸ“„ PDF</a>
                </div>
            </div>
            

            <div class="paper-card">
                <div class="paper-title">
                    <a href="https://arxiv.org/abs/2602.13191v1" target="_blank">CoPE-VideoLM: Codec Primitives For Efficient Video Language Models</a>
                </div>
                <div class="paper-authors">ğŸ‘¥ Sayan Deb Sarkar, RÃ©mi Pautrat, Ondrej Miksik</div>
                <div class="paper-summary">Video Language Models (VideoLMs) empower AI systems to understand temporal dynamics in videos. To fit to the maximum context window constraint, current methods use keyframe sampling which can miss both macro-level events and micro-level details due to the sparse temporal coverage. Furthermore, processing full images and their tokens for each frame incurs substantial computational overhead. To address these limitations, we propose to leverage video codec primitives (specifically motion vectors and residuals) which natively encode video redundancy and sparsity without requiring expensive full-image encoding for most frames. To this end, we introduce lightweight transformer-based encoders that aggregate codec primitives and align their representations with image encoder embeddings through a pre-training strategy that accelerates convergence during end-to-end fine-tuning. Our approach reduces the time-to-first-token by up to $86\%$ and token usage by up to $93\%$ compared to standard VideoLMs. Moreover, by varying the keyframe and codec primitive densities we are able to maintain or exceed performance on $14$ diverse video understanding benchmarks spanning general question answering, temporal reasoning, long-form understanding, and spatial scene understanding.</div>
                <div class="paper-meta">
                    <span class="paper-tag">ğŸ“… 2026-02-13</span>
                    <a href="https://arxiv.org/pdf/2602.13191v1" class="paper-link" target="_blank">ğŸ“„ PDF</a>
                </div>
            </div>
            

            <div class="paper-card">
                <div class="paper-title">
                    <a href="https://arxiv.org/abs/2602.13151v1" target="_blank">Quantization-Robust LLM Unlearning via Low-Rank Adaptation</a>
                </div>
                <div class="paper-authors">ğŸ‘¥ JoÃ£o Vitor Boer Abitante, Joana Meneguzzo Pasquali, Luan Fonseca Garcia</div>
                <div class="paper-summary">Large Language Model (LLM) unlearning aims to remove targeted knowledge from a trained model, but practical deployments often require post-training quantization (PTQ) for efficient inference. However, aggressive low-bit PTQ can mask or erase unlearning updates, causing quantized models to revert to pre-unlearning behavior. We show that standard full-parameter fine-tuning often induce parameter changes that are too small to survive 4-bit quantization. We propose quantization-robust unlearning via low-rank adaptation (LoRA): we freeze the base model and concentrate unlearning into trainable adapters so that the effective update is preserved after quantization. On Llama-2-7B evaluated with MUSE dataset (BOOKS and NEWS), LoRA improves 4-bit utility by up to 7.93 points (NPO+GDR on BOOKS: 50.17 to 58.10) and yields higher 4-bit utility on NEWS for GA+GDR (40.06 to 44.82, increase of 4.76). LoRA also substantially reduces privacy leakage under 4-bit PTQ, e.g., for GA+KLR on BOOKS, PrivLeak moves from -25.68 to -5.86 (closer to ideal 0), while maintaining strong forgetting (VerMem and KnowMem near 0). Thus, using LoRA for Machine Unlearning is beneficial for scenarios where quantization is necessary for model deployment.</div>
                <div class="paper-meta">
                    <span class="paper-tag">ğŸ“… 2026-02-13</span>
                    <a href="https://arxiv.org/pdf/2602.13151v1" class="paper-link" target="_blank">ğŸ“„ PDF</a>
                </div>
            </div>
            

            <div class="paper-card">
                <div class="paper-title">
                    <a href="https://arxiv.org/abs/2602.13110v1" target="_blank">SCOPE: Selective Conformal Optimized Pairwise LLM Judging</a>
                </div>
                <div class="paper-authors">ğŸ‘¥ Sher Badshah, Ali Emami, Hassan Sajjad</div>
                <div class="paper-summary">Large language models (LLMs) are increasingly used as judges to replace costly human preference labels in pairwise evaluation. Despite their practicality, LLM judges remain prone to miscalibration and systematic biases. This paper proposes SCOPE (Selective Conformal Optimized Pairwise Evaluation), a framework for selective pairwise judging with finite-sample statistical guarantees. Under exchangeability, SCOPE calibrates an acceptance threshold such that the error rate among non-abstained judgments is at most a user-specified level $Î±$. To provide SCOPE with a bias-neutral uncertainty signal, we introduce Bidirectional Preference Entropy (BPE), which queries the judge under both response positions, aggregates the implied preference probabilities to enforce invariance to response order, and converts the aggregated probability into an entropy-based uncertainty score. Across MT-Bench, RewardBench, and Chatbot Arena, BPE improves uncertainty quality over standard confidence proxies, providing a stronger selection signal that enables SCOPE to consistently meet the target risk level while retaining good coverage across judge scales. In particular, at $Î±= 0.10$, \textsc{Scope} consistently satisfies the risk bound across all benchmarks and judge scales (empirical risk $\approx 0.097$ to $0.099$), while retaining substantial coverage, reaching $0.89$ on RewardBench with Qwen-14B and $0.98$ on RewardBench with Qwen-32B. Compared to naÃ¯ve baselines, \textsc{Scope} accepts up to $2.4\times$ more judgments on MT-Bench with Qwen-7B under the same target risk constraint, demonstrating that BPE enables reliable and high-coverage LLM-based evaluation.</div>
                <div class="paper-meta">
                    <span class="paper-tag">ğŸ“… 2026-02-13</span>
                    <a href="https://arxiv.org/pdf/2602.13110v1" class="paper-link" target="_blank">ğŸ“„ PDF</a>
                </div>
            </div>
            

            <div class="paper-card">
                <div class="paper-title">
                    <a href="https://arxiv.org/abs/2602.13093v1" target="_blank">Consistency of Large Reasoning Models Under Multi-Turn Attacks</a>
                </div>
                <div class="paper-authors">ğŸ‘¥ Yubo Li, Ramayya Krishnan, Rema Padman</div>
                <div class="paper-summary">Large reasoning models with reasoning capabilities achieve state-of-the-art performance on complex tasks, but their robustness under multi-turn adversarial pressure remains underexplored. We evaluate nine frontier reasoning models under adversarial attacks. Our findings reveal that reasoning confers meaningful but incomplete robustness: most reasoning models studied significantly outperform instruction-tuned baselines, yet all exhibit distinct vulnerability profiles, with misleading suggestions universally effective and social pressure showing model-specific efficacy. Through trajectory analysis, we identify five failure modes (Self-Doubt, Social Conformity, Suggestion Hijacking, Emotional Susceptibility, and Reasoning Fatigue) with the first two accounting for 50% of failures. We further demonstrate that Confidence-Aware Response Generation (CARG), effective for standard LLMs, fails for reasoning models due to overconfidence induced by extended reasoning traces; counterintuitively, random confidence embedding outperforms targeted extraction. Our results highlight that reasoning capabilities do not automatically confer adversarial robustness and that confidence-based defenses require fundamental redesign for reasoning models.</div>
                <div class="paper-meta">
                    <span class="paper-tag">ğŸ“… 2026-02-13</span>
                    <a href="https://arxiv.org/pdf/2602.13093v1" class="paper-link" target="_blank">ğŸ“„ PDF</a>
                </div>
            </div>
            

            <div class="paper-card">
                <div class="paper-title">
                    <a href="https://arxiv.org/abs/2602.13084v1" target="_blank">Exploring a New Competency Modeling Process with Large Language Models</a>
                </div>
                <div class="paper-authors">ğŸ‘¥ Silin Du, Manqing Xin, Raymond Jia Wang</div>
                <div class="paper-summary">Competency modeling is widely used in human resource management to select, develop, and evaluate talent. However, traditional expert-driven approaches rely heavily on manual analysis of large volumes of interview transcripts, making them costly and prone to randomness, ambiguity, and limited reproducibility. This study proposes a new competency modeling process built on large language models (LLMs). Instead of merely automating isolated steps, we reconstruct the workflow by decomposing expert practices into structured computational components. Specifically, we leverage LLMs to extract behavioral and psychological descriptions from raw textual data and map them to predefined competency libraries through embedding-based similarity. We further introduce a learnable parameter that adaptively integrates different information sources, enabling the model to determine the relative importance of behavioral and psychological signals. To address the long-standing challenge of validation, we develop an offline evaluation procedure that allows systematic model selection without requiring additional large-scale data collection. Empirical results from a real-world implementation in a software outsourcing company demonstrate strong predictive validity, cross-library consistency, and structural robustness. Overall, our framework transforms competency modeling from a largely qualitative and expert-dependent practice into a transparent, data-driven, and evaluable analytical process.</div>
                <div class="paper-meta">
                    <span class="paper-tag">ğŸ“… 2026-02-13</span>
                    <a href="https://arxiv.org/pdf/2602.13084v1" class="paper-link" target="_blank">ğŸ“„ PDF</a>
                </div>
            </div>
            
            
            <h3 style="color: #4a5568; margin-top:30px;">ğŸ“Š å¹¿å‘Šä¸æ¨èç®—æ³•</h3>
            
            <div class="paper-card ad">
                <div class="paper-title">
                    <a href="https://arxiv.org/abs/2602.12593v1" target="_blank">RQ-GMM: Residual Quantized Gaussian Mixture Model for Multimodal Semantic Discretization in CTR Prediction</a>
                </div>
                <div class="paper-authors">ğŸ‘¥ Ziye Tong, Jiahao Liu, Weimin Zhang</div>
                <div class="paper-summary">Multimodal content is crucial for click-through rate (CTR) prediction. However, directly incorporating continuous embeddings from pre-trained models into CTR models yields suboptimal results due to misaligned optimization objectives and convergence speed inconsistency during joint training. Discretizing embeddings into semantic IDs before feeding them into CTR models offers a more effective solution, yet existing methods suffer from limited codebook utilization, reconstruction accuracy, and semantic discriminability. We propose RQ-GMM (Residual Quantized Gaussian Mixture Model), which introduces probabilistic modeling to better capture the statistical structure of multimodal embedding spaces. Through Gaussian Mixture Models combined with residual quantization, RQ-GMM achieves superior codebook utilization and reconstruction accuracy. Experiments on public datasets and online A/B tests on a large-scale short-video platform serving hundreds of millions of users demonstrate substantial improvements: RQ-GMM yields a 1.502% gain in Advertiser Value over strong baselines. The method has been fully deployed, serving daily recommendations for hundreds of millions of users.</div>
                <div class="paper-meta">
                    <span class="paper-tag">ğŸ“… 2026-02-13</span>
                    <a href="https://arxiv.org/pdf/2602.12593v1" class="paper-link" target="_blank">ğŸ“„ PDF</a>
                </div>
            </div>
            

            <div class="paper-card ad">
                <div class="paper-title">
                    <a href="https://arxiv.org/abs/2602.12041v1" target="_blank">Compress, Cross and Scale: Multi-Level Compression Cross Networks for Efficient Scaling in Recommender Systems</a>
                </div>
                <div class="paper-authors">ğŸ‘¥ Heng Yu, Xiangjun Zhou, Jie Xia</div>
                <div class="paper-summary">Modeling high-order feature interactions efficiently is a central challenge in click-through rate and conversion rate prediction. Modern industrial recommender systems are predominantly built upon deep learning recommendation models, where the interaction backbone plays a critical role in determining both predictive performance and system efficiency. However, existing interaction modules often struggle to simultaneously achieve strong interaction capacity, high computational efficiency, and good scalability, resulting in limited ROI when models are scaled under strict production constraints. In this work, we propose MLCC, a structured feature interaction architecture that organizes feature crosses through hierarchical compression and dynamic composition, which can efficiently capture high-order feature dependencies while maintaining favorable computational complexity. We further introduce MC-MLCC, a Multi-Channel extension that decomposes feature interactions into parallel subspaces, enabling efficient horizontal scaling with improved representation capacity and significantly reduced parameter growth. Extensive experiments on three public benchmarks and a large-scale industrial dataset show that our proposed models consistently outperform strong DLRM-style baselines by up to 0.52 AUC, while reducing model parameters and FLOPs by up to 26$\times$ under comparable performance. Comprehensive scaling analyses demonstrate stable and predictable scaling behavior across embedding dimension, head number, and channel count, with channel-based scaling achieving substantially better efficiency than conventional embedding inflation. Finally, online A/B testing on a real-world advertising platform validates the practical effectiveness of our approach, which has been widely adopted in Bilibili advertising system under strict latency and resource constraints.</div>
                <div class="paper-meta">
                    <span class="paper-tag">ğŸ“… 2026-02-12</span>
                    <a href="https://arxiv.org/pdf/2602.12041v1" class="paper-link" target="_blank">ğŸ“„ PDF</a>
                </div>
            </div>
            

            <div class="paper-card ad">
                <div class="paper-title">
                    <a href="https://arxiv.org/abs/2602.10811v1" target="_blank">EST: Towards Efficient Scaling Laws in Click-Through Rate Prediction via Unified Modeling</a>
                </div>
                <div class="paper-authors">ğŸ‘¥ Mingyang Liu, Yong Bai, Zhangming Chan</div>
                <div class="paper-summary">Efficiently scaling industrial Click-Through Rate (CTR) prediction has recently attracted significant research attention. Existing approaches typically employ early aggregation of user behaviors to maintain efficiency. However, such non-unified or partially unified modeling creates an information bottleneck by discarding fine-grained, token-level signals essential for unlocking scaling gains. In this work, we revisit the fundamental distinctions between CTR prediction and Large Language Models (LLMs), identifying two critical properties: the asymmetry in information density between behavioral and non-behavioral features, and the modality-specific priors of content-rich signals. Accordingly, we propose the Efficiently Scalable Transformer (EST), which achieves fully unified modeling by processing all raw inputs in a single sequence without lossy aggregation. EST integrates two modules: Lightweight Cross-Attention (LCA), which prunes redundant self-interactions to focus on high-impact cross-feature dependencies, and Content Sparse Attention (CSA), which utilizes content similarity to dynamically select high-signal behaviors. Extensive experiments show that EST exhibits a stable and efficient power-law scaling relationship, enabling predictable performance gains with model scale. Deployed on Taobao's display advertising platform, EST significantly outperforms production baselines, delivering a 3.27\% RPM (Revenue Per Mile) increase and a 1.22\% CTR lift, establishing a practical pathway for scalable industrial CTR prediction models.</div>
                <div class="paper-meta">
                    <span class="paper-tag">ğŸ“… 2026-02-11</span>
                    <a href="https://arxiv.org/pdf/2602.10811v1" class="paper-link" target="_blank">ğŸ“„ PDF</a>
                </div>
            </div>
            

            <div class="paper-card ad">
                <div class="paper-title">
                    <a href="https://arxiv.org/abs/2602.12972v1" target="_blank">Jointly Optimizing Debiased CTR and Uplift for Coupons Marketing: A Unified Causal Framework</a>
                </div>
                <div class="paper-authors">ğŸ‘¥ Siyun Yang, Shixiao Yang, Jian Wang</div>
                <div class="paper-summary">In online advertising, marketing interventions such as coupons introduce significant confounding bias into Click-Through Rate (CTR) prediction. Observed clicks reflect a mixture of users' intrinsic preferences and the uplift induced by these interventions. This causes conventional models to miscalibrate base CTRs, which distorts downstream ranking and billing decisions. Furthermore, marketing interventions often operate as multi-valued treatments with varying magnitudes, introducing additional complexity to CTR prediction.   To address these issues, we propose the \textbf{Uni}fied \textbf{M}ulti-\textbf{V}alued \textbf{T}reatment Network (UniMVT). Specifically, UniMVT disentangles confounding factors from treatment-sensitive representations, enabling a full-space counterfactual inference module to jointly reconstruct the debiased base CTR and intensity-response curves. To handle the complexity of multi-valued treatments, UniMVT employs an auxiliary intensity estimation task to capture treatment propensities and devise a unit uplift objective that normalizes the intervention effect. This ensures comparable estimation across the continuous coupon-value spectrum. UniMVT simultaneously achieves debiased CTR prediction for accurate system calibration and precise uplift estimation for incentive allocation. Extensive experiments on synthetic and industrial datasets demonstrate UniMVT's superiority in both predictive accuracy and calibration. Furthermore, real-world A/B tests confirm that UniMVT significantly improves business metrics through more effective coupon distribution.</div>
                <div class="paper-meta">
                    <span class="paper-tag">ğŸ“… 2026-02-13</span>
                    <a href="https://arxiv.org/pdf/2602.12972v1" class="paper-link" target="_blank">ğŸ“„ PDF</a>
                </div>
            </div>
            

            <div class="paper-card ad">
                <div class="paper-title">
                    <a href="https://arxiv.org/abs/2602.11410v1" target="_blank">CADET: Context-Conditioned Ads CTR Prediction With a Decoder-Only Transformer</a>
                </div>
                <div class="paper-authors">ğŸ‘¥ David Pardoe, Neil Daftary, Miro Furtado</div>
                <div class="paper-summary">Click-through rate (CTR) prediction is fundamental to online advertising systems. While Deep Learning Recommendation Models (DLRMs) with explicit feature interactions have long dominated this domain, recent advances in generative recommenders have shown promising results in content recommendation. However, adapting these transformer-based architectures to ads CTR prediction still presents unique challenges, including handling post-scoring contextual signals, maintaining offline-online consistency, and scaling to industrial workloads. We present CADET (Context-Conditioned Ads Decoder-Only Transformer), an end-to-end decoder-only transformer for ads CTR prediction deployed at LinkedIn. Our approach introduces several key innovations: (1) a context-conditioned decoding architecture with multi-tower prediction heads that explicitly model post-scoring signals such as ad position, resolving the chicken-and-egg problem between predicted CTR and ranking; (2) a self-gated attention mechanism that stabilizes training by adaptively regulating information flow at both representation and interaction levels; (3) a timestamp-based variant of Rotary Position Embedding (RoPE) that captures temporal relationships across timescales from seconds to months; (4) session masking strategies that prevent the model from learning dependencies on unavailable in-session events, addressing train-serve skew; and (5) production engineering techniques including tensor packing, sequence chunking, and custom Flash Attention kernels that enable efficient training and serving at scale. In online A/B testing, CADET achieves a 11.04\% CTR lift compared to the production LiRank baseline model, a hybrid ensemble of DCNv2 and sequential encoders. The system has been successfully deployed on LinkedIn's advertising platform, serving the main traffic for homefeed sponsored updates.</div>
                <div class="paper-meta">
                    <span class="paper-tag">ğŸ“… 2026-02-11</span>
                    <a href="https://arxiv.org/pdf/2602.11410v1" class="paper-link" target="_blank">ğŸ“„ PDF</a>
                </div>
            </div>
            
        </div>
        
        <!-- RSS Feeds -->
        <div class="section">
            <div class="section-header">
                <span class="section-icon">ğŸ“¡</span>
                <h2 class="section-title">ä¸šç•ŒåŠ¨æ€</h2>
            </div>
            
            <div style="display: grid; grid-template-columns: 1fr 1fr; gap: 40px;">
                <div>
                    <h3 style="border-bottom: 2px solid #ed8936; padding-bottom: 10px; color: #c05621;">ğŸ¢ AI Labs æ›´æ–°</h3>
                    <p style="color: #cbd5e0;">æš‚æ— åŠ¨æ€</p>
                </div>
                <div>
                    <h3 style="border-bottom: 2px solid #48bb78; padding-bottom: 10px; color: #2f855a;">ğŸ’° é¡¶çº§é£æŠ•è§‚ç‚¹</h3>
                    <p style="color: #cbd5e0;">æš‚æ— åŠ¨æ€</p>
                </div>
            </div>
            
            <div style="margin-top: 40px;">
                 <h3 style="border-bottom: 2px solid #4299e1; padding-bottom: 10px; color: #2b6cb0;">ğŸ“° ç§‘æŠ€æ–°é—»ç²¾é€‰</h3>
                 <div class="feed-list">
            <div class="feed-item">
                <div class="feed-source">TechCrunch</div>
                <div class="feed-title"><a href="https://techcrunch.com/2026/02/15/as-ai-data-centers-hit-power-limits-peak-xv-backs-indian-startup-c2i-to-fix-the-bottleneck/" target="_blank">As AI data centers hit power limits, Peak XV backs Indian startup C2i to fix the bottleneck</a></div>
                <div class="feed-date">02-16</div>
            </div>
            
            <div class="feed-item">
                <div class="feed-source">TechCrunch AI</div>
                <div class="feed-title"><a href="https://techcrunch.com/2026/02/15/as-ai-data-centers-hit-power-limits-peak-xv-backs-indian-startup-c2i-to-fix-the-bottleneck/" target="_blank">As AI data centers hit power limits, Peak XV backs Indian startup C2i to fix the bottleneck</a></div>
                <div class="feed-date">02-16</div>
            </div>
            
            <div class="feed-item">
                <div class="feed-source">TechCrunch</div>
                <div class="feed-title"><a href="https://techcrunch.com/2026/02/15/blackstone-backs-neysa-in-up-to-1-2b-financing-as-india-pushes-to-build-domestic-ai-compute/" target="_blank">Blackstone backs Neysa in up to $1.2B financing as India pushes to build domestic AI infrastructure</a></div>
                <div class="feed-date">02-16</div>
            </div>
            
            <div class="feed-item">
                <div class="feed-source">TechCrunch AI</div>
                <div class="feed-title"><a href="https://techcrunch.com/2026/02/15/blackstone-backs-neysa-in-up-to-1-2b-financing-as-india-pushes-to-build-domestic-ai-compute/" target="_blank">Blackstone backs Neysa in up to $1.2B financing as India pushes to build domestic AI infrastructure</a></div>
                <div class="feed-date">02-16</div>
            </div>
            
            <div class="feed-item">
                <div class="feed-source">The Verge</div>
                <div class="feed-title"><a href="https://www.theverge.com/ai-artificial-intelligence/879623/openclaw-founder-peter-steinberger-joins-openai" target="_blank">OpenClaw founder Peter Steinberger is joining OpenAI</a></div>
                <div class="feed-date">02-15</div>
            </div>
            
            <div class="feed-item">
                <div class="feed-source">TechCrunch</div>
                <div class="feed-title"><a href="https://techcrunch.com/2026/02/15/openclaw-creator-peter-steinberger-joins-openai/" target="_blank">OpenClaw creator Peter Steinberger joins OpenAI</a></div>
                <div class="feed-date">02-15</div>
            </div>
            
            <div class="feed-item">
                <div class="feed-source">TechCrunch AI</div>
                <div class="feed-title"><a href="https://techcrunch.com/2026/02/15/openclaw-creator-peter-steinberger-joins-openai/" target="_blank">OpenClaw creator Peter Steinberger joins OpenAI</a></div>
                <div class="feed-date">02-15</div>
            </div>
            
            <div class="feed-item">
                <div class="feed-source">TechCrunch</div>
                <div class="feed-title"><a href="https://techcrunch.com/2026/02/15/longtime-npr-host-david-greene-sues-google-over-notebooklm-voice/" target="_blank">Longtime NPR host David Greene sues Google over NotebookLM voice</a></div>
                <div class="feed-date">02-15</div>
            </div>
            </div>
            </div>
        </div>
        
        <!-- YouTube Videos -->
        <div class="section">
            <div class="section-header">
                <span class="section-icon">ğŸ¬</span>
                <h2 class="section-title">ç§‘æŠ€é¢†è¢–è®¿è°ˆ</h2>
            </div>
            <h4 style="margin: 20px 0 10px 0; color: #553c9a;">ğŸ‘¤ Elon Musk</h4>

                <div class="video-card">
                    <div class="video-title"><a href="https://www.youtube.com/watch?v=IgifEgm1-e0" target="_blank">ğŸ¥ Conversation with Elon Musk | World Economic Forum Annual Meeting 2026</a></div>
                    <div style="font-size: 12px; color: #a0aec0; margin-top:5px;">Please join us for a conversation between Elon Musk, CEO of Tesla; Chief Engineer, SpaceX; CTO, xAI;...</div>
                </div>
                

                <div class="video-card">
                    <div class="video-title"><a href="https://www.youtube.com/watch?v=UrB2tQDVLLo" target="_blank">ğŸ¥ Tesla CEO Elon Musk speaks at the World Economic Forum in Davos, Switzerland â€” 1/22/2026</a></div>
                    <div style="font-size: 12px; color: #a0aec0; margin-top:5px;">Elon Musk, CEO of Tesla; chief engineer of SpaceX; and CTO of xAI joins Laurence D. Fink, chair and ...</div>
                </div>
                
<h4 style="margin: 20px 0 10px 0; color: #553c9a;">ğŸ‘¤ Jensen Huang</h4>

                <div class="video-card">
                    <div class="video-title"><a href="https://www.youtube.com/watch?v=6fbyiPRhMSs" target="_blank">ğŸ¥ Cisco AI Summit | Special live event with Jensen Huang</a></div>
                    <div style="font-size: 12px; color: #a0aec0; margin-top:5px;">Live from the Cisco AI Summit, Chuck Robbins, Chair & CEO of Cisco and Jensen Huang, Founder, Presid...</div>
                </div>
                

                <div class="video-card">
                    <div class="video-title"><a href="https://www.youtube.com/watch?v=0NBILspM4c4" target="_blank">ğŸ¥ NVIDIA Live with CEO Jensen Huang</a></div>
                    <div style="font-size: 12px; color: #a0aec0; margin-top:5px;">Live from CES in Las Vegas, NVIDIA CEO Jensen Huang shares how the next generation of accelerated co...</div>
                </div>
                
<h4 style="margin: 20px 0 10px 0; color: #553c9a;">ğŸ‘¤ Sam Altman</h4>

                <div class="video-card">
                    <div class="video-title"><a href="https://www.youtube.com/watch?v=SIP83fcFzSA" target="_blank">ğŸ¥ Sam Altman OpenAI Stanford Treehacks Opening Ceremony 2026</a></div>
                    <div style="font-size: 12px; color: #a0aec0; margin-top:5px;">...</div>
                </div>
                

                <div class="video-card">
                    <div class="video-title"><a href="https://www.youtube.com/watch?v=adzz3gO5OsM" target="_blank">ğŸ¥ Opening Ceremony | TreeHacks 2026</a></div>
                    <div style="font-size: 12px; color: #a0aec0; margin-top:5px;">Welcome to the official Opening Ceremony of TreeHacks 2026, Stanford's premier hackathon. This weeke...</div>
                </div>
                
        </div>
        
        <!-- New Sources: GitHub, Reddit, HN -->
        
            <div class="section">
                <div class="section-header">
                    <span class="section-icon">ğŸŒ</span>
                    <h2 class="section-title">ç¤¾åŒºç²¾é€‰ (AI Curated)</h2>
                </div>
                
            <div style="margin-bottom: 30px;">
                <h3 style="border-bottom: 2px solid #f59e0b; padding-bottom: 10px; color: #d97706;">ğŸ”¥ GitHub è¶‹åŠ¿é¡¹ç›®</h3>
                
                <div style="padding: 10px; border-left: 3px solid #cbd5e0; margin: 8px 0;">
                    <a href="https://github.com/zeroclaw-labs/zeroclaw" target="_blank" style="color: #2b6cb0;">
                        zeroclaw-labs/zeroclaw
                    </a>
                </div>
                

                <div style="padding: 10px; border-left: 3px solid #cbd5e0; margin: 8px 0;">
                    <a href="https://github.com/bwya77/vscode-dark-islands" target="_blank" style="color: #2b6cb0;">
                        bwya77/vscode-dark-islands
                    </a>
                </div>
                

                <div style="padding: 10px; border-left: 3px solid #cbd5e0; margin: 8px 0;">
                    <a href="https://github.com/HKUDS/FastCode" target="_blank" style="color: #2b6cb0;">
                        HKUDS/FastCode
                    </a>
                </div>
                
            </div>
            
            <div style="margin-bottom: 30px;">
                <h3 style="border-bottom: 2px solid #f97316; padding-bottom: 10px; color: #ea580c;">ğŸŸ  Hacker News ç²¾é€‰</h3>
                
                <div style="padding: 10px; border-left: 3px solid #cbd5e0; margin: 8px 0;">
                    <a href="https://news.ycombinator.com/item?id=47028013" target="_blank" style="color: #2b6cb0;">
                        Iâ€™m joining OpenAI
                    </a>
                </div>
                

                <div style="padding: 10px; border-left: 3px solid #cbd5e0; margin: 8px 0;">
                    <a href="https://news.ycombinator.com/item?id=47004384" target="_blank" style="color: #2b6cb0;">
                        I gave Claude access to my pen plotter
                    </a>
                </div>
                

                <div style="padding: 10px; border-left: 3px solid #cbd5e0; margin: 8px 0;">
                    <a href="https://news.ycombinator.com/item?id=47026186" target="_blank" style="color: #2b6cb0;">
                        Show HN: Microgpt is a GPT you can visualize in the browser
                    </a>
                </div>
                
            </div>
            
            </div>
            
        
        
        <div class="stats-box">
            <h4 style="margin:0 0 10px 0; color:#4a5568;">âš™ï¸ ç³»ç»Ÿè¿è¡Œç»Ÿè®¡</h4>
            <div style="display:flex; justify-content:space-between; font-size:12px; color:#718096;">
                <div>
                    <strong>ğŸ¤– AI æ¨¡å‹ (Qwen-72B)</strong><br>
                    è°ƒç”¨æ¬¡æ•°: 0<br>
                    Input Tokens: 0<br>
                    Output Tokens: 0
                </div>
                <div>
                    <strong>ğŸ¬ YouTube API</strong><br>
                    API è°ƒç”¨: 17<br>
                    Quota æ¶ˆè€—: 908 units
                </div>
            </div>
        </div>
        
        
        <div class="footer">
            <p>ğŸ“… 2026å¹´02æœˆ16æ—¥ | Daily Info System</p>
            <p>ğŸ’¡ Stay Hungry, Stay Foolish</p>
        </div>
    </div>
</body>
</html>
