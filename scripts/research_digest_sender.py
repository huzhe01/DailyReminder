#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
AI ç ”ç©¶æ‘˜è¦é‚®ä»¶æ¨é€
æ•´åˆ arXiv è®ºæ–‡ã€YouTubeã€RSSã€GitHubã€Redditã€HNï¼Œå‘é€æ¯æ—¥ç ”ç©¶æ‘˜è¦
"""

import os
import smtplib
from email.mime.text import MIMEText
from email.mime.multipart import MIMEMultipart
from email.header import Header
from datetime import datetime
from typing import List, Dict, Optional
from openai import OpenAI
import time

from arxiv_fetcher import ArxivFetcher, ArxivPaper, filter_recent_papers
from youtube_fetcher import YouTubeFetcher, YouTubeFetcherNoAPI, YouTubeVideo
from feed_fetcher import FeedFetcher, FeedItem
from github_fetcher import GitHubFetcher, GitHubIssue, TrendingRepo
from reddit_fetcher import RedditFetcher, RedditPost
from hn_fetcher import HNFetcher, HNStory
from deduplicator import Deduplicator
from ai_curator import AICurator

class UsageTracker:
    """èµ„æºä½¿ç”¨ç»Ÿè®¡è¿½è¸ªå™¨"""
    def __init__(self):
        self.llm_calls = 0
        self.llm_input_tokens = 0
        self.llm_output_tokens = 0
        self.youtube_api_calls = 0
        self.youtube_quota = 0
    
    def log_llm_usage(self, usage):
        """è®°å½• LLM Token ä½¿ç”¨"""
        if usage:
            self.llm_calls += 1
            self.llm_input_tokens += getattr(usage, 'prompt_tokens', 0)
            self.llm_output_tokens += getattr(usage, 'completion_tokens', 0)
            
    def log_youtube_usage(self, calls: int, quota: int):
        """è®°å½• YouTube API ä½¿ç”¨"""
        self.youtube_api_calls += calls
        self.youtube_quota += quota

class ResearchDigestSender:
    """AI ç ”ç©¶æ‘˜è¦é‚®ä»¶å‘é€å™¨"""
    
    def __init__(self):
        self.arxiv_fetcher = ArxivFetcher(max_results=15)
        self.feed_fetcher = FeedFetcher(days_lookback=2)
        self.github_fetcher = GitHubFetcher()
        self.reddit_fetcher = RedditFetcher()
        self.hn_fetcher = HNFetcher()
        self.deduplicator = Deduplicator()
        self.youtube_api_key = os.getenv('YOUTUBE_API_KEY')
        self.usage_tracker = UsageTracker()
        
        # Initialize OpenAI client for paper summarization
        api_key = os.getenv('MODELSCOPE_API_KEY')
        if not api_key:
            raise ValueError("è¯·è®¾ç½® MODELSCOPE_API_KEY ç¯å¢ƒå˜é‡")
        self.client = OpenAI(
            base_url='https://api-inference.modelscope.cn/v1/',
            api_key=api_key
        )
        
        # AI Curator (shares the same client)
        self.ai_curator = AICurator(client=self.client)
        
        if self.youtube_api_key:
            self.youtube_fetcher = YouTubeFetcher(api_key=self.youtube_api_key, max_results=5)
        else:
            self.youtube_fetcher = None
            self.youtube_no_api = YouTubeFetcherNoAPI()
        
        # é€‰æ‹©è¦å…³æ³¨çš„ç§‘æŠ€é¢†è¢–
        self.selected_leaders = ["Elon Musk", "Jensen Huang", "Sam Altman"]
    
    def summarize_paper(self, title: str, abstract: str) -> str:
        """ä½¿ç”¨ AI ç¿»è¯‘å¹¶æ€»ç»“è®ºæ–‡"""
        try:
            response = self.client.chat.completions.create(
                model="Qwen/Qwen3-32B",
                messages=[
                    {"role": "system", "content": "ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„AIç ”ç©¶åŠ©æ‰‹ã€‚è¯·å°†ç»™å®šçš„è®ºæ–‡æ‘˜è¦ç¿»è¯‘æˆä¸­æ–‡ï¼Œå¹¶ç”¨ä¸€å¥è¯æ€»ç»“è¿™ç¯‡è®ºæ–‡çš„æ ¸å¿ƒè´¡çŒ®ã€‚æ ¼å¼è¦æ±‚ï¼šå…ˆç»™å‡ºä¸­æ–‡æ‘˜è¦ï¼Œæ¢è¡Œåç»™å‡º'æ ¸å¿ƒè´¡çŒ®ï¼š'ã€‚"},
                    {"role": "user", "content": f"Title: {title}\nAbstract: {abstract}"}
                ],
                extra_body={"enable_thinking": False}
            )
            self.usage_tracker.log_llm_usage(response.usage)
            return response.choices[0].message.content
        except Exception as e:
            print(f"âŒ AI æ‘˜è¦ç”Ÿæˆå¤±è´¥: {e}")
            return abstract

    def generate_daily_briefing(self, papers: Dict, feeds: Dict, videos: Dict) -> str:
        """ç”Ÿæˆæ¯æ—¥ AI ç®€æŠ¥"""
        print("\nğŸ¤– æ­£åœ¨ç”Ÿæˆæ¯æ—¥ AI ç®€æŠ¥...")
        
        # å‡†å¤‡è¾“å…¥æ•°æ® provided to LLM
        context = "è¯·æ ¹æ®ä»¥ä¸‹ä»Šå¤©æ”¶é›†åˆ°çš„ä¿¡æ¯ï¼Œä¸ºæˆ‘æ’°å†™ä¸€ä»½ç®€çŸ­çš„'æ¯æ—¥ AI ç®€æŠ¥' (Daily Briefing)ã€‚\n\n"
        
        # Top 3 LLM Papers
        context += "ã€çƒ­é—¨å¤§æ¨¡å‹è®ºæ–‡ã€‘\n"
        for p in papers.get('llm', [])[:3]:
            context += f"- {p.title}\n"
            
        # Top News
        context += "\nã€é‡è¦ç§‘æŠ€æ–°é—»ã€‘\n"
        news_items = feeds.get('Tech_News', [])[:3] + feeds.get('AI_Labs', [])[:3]
        for item in news_items:
            context += f"- {item.title} ({item.source_name})\n"
            
        # Top Videos
        context += "\nã€æœ€æ–°è®¿è°ˆã€‘\n"
        video_data = videos.get('data', {})
        if videos['type'] == 'api':
            for leader, vids in video_data.items():
                if vids:
                    context += f"- {leader}: {vids[0].title}\n"
        
        context += "\nè¦æ±‚ï¼šç”¨ä¸­æ–‡æ’°å†™ï¼Œè¯­æ°”ä¸“ä¸šä¸”å¼•äººå…¥èƒœã€‚åˆ†ä¸ºä¸‰ä¸ªç®€çŸ­æ®µè½ï¼š1. å­¦æœ¯çªç ´ (åŸºäºè®ºæ–‡); 2. è¡Œä¸šåŠ¨æ€ (åŸºäºæ–°é—»); 3. å€¼å¾—å…³æ³¨ (ç»¼åˆ)ã€‚æ€»å­—æ•°æ§åˆ¶åœ¨ 400 å­—ä»¥å†…ã€‚"

        try:
            response = self.client.chat.completions.create(
                model="Qwen/Qwen3-32B",
                messages=[
                    {"role": "system", "content": "ä½ æ˜¯ä¸€ä½èµ„æ·±çš„ç§‘æŠ€ä¸»ç¼–ï¼Œæ“…é•¿ä»æµ·é‡ä¿¡æ¯ä¸­æç‚¼å…³é”®æ´å¯Ÿã€‚"},
                    {"role": "user", "content": context}
                ],
                extra_body={"enable_thinking": False}
            )
            self.usage_tracker.log_llm_usage(response.usage)
            return response.choices[0].message.content
        except Exception as e:
            print(f"âŒ ç®€æŠ¥ç”Ÿæˆå¤±è´¥: {e}")
            return "æ— æ³•ç”Ÿæˆä»Šæ—¥ç®€æŠ¥ï¼Œè¯·ç›´æ¥é˜…è¯»ä¸‹æ–¹è¯¦ç»†å†…å®¹ã€‚"

    def fetch_arxiv_papers(self) -> Dict[str, List[ArxivPaper]]:
        """è·å– arXiv è®ºæ–‡"""
        print("\n" + "=" * 60)
        print("ğŸ“š æ­£åœ¨è·å– arXiv è®ºæ–‡...")
        print("=" * 60)
        
        papers = self.arxiv_fetcher.fetch_all()
        
        # è¿‡æ»¤æœ€è¿‘ 7 å¤©çš„è®ºæ–‡
        for category in papers:
            papers[category] = filter_recent_papers(papers[category], days=7)
            
            # AI æ‘˜è¦ç”Ÿæˆ (åªå¤„ç†å‰5ç¯‡ä»¥èŠ‚çœèµ„æºå’Œæ—¶é—´)
            if papers[category]:
                print(f"\nğŸ¤– æ­£åœ¨ç”Ÿæˆ {category} ç±»åˆ«çš„ AI æ‘˜è¦...")
                for i, paper in enumerate(papers[category][:5]):
                    print(f"  [{i+1}/{min(len(papers[category]), 5)}] å¤„ç†: {paper.title[:30]}...")
                    paper.summary = self.summarize_paper(paper.title, paper.summary)
        
        print(f"âœ… è·å–å®Œæˆ: {len(papers['llm'])} ç¯‡å¤§æ¨¡å‹è®ºæ–‡, {len(papers['advertising'])} ç¯‡å¹¿å‘Šé¢†åŸŸè®ºæ–‡")
        return papers
    
    def fetch_feeds(self) -> Dict[str, List[FeedItem]]:
        """è·å– RSS è®¢é˜…"""
        print("\n" + "=" * 60)
        print("rss æ­£åœ¨è·å– RSS è®¢é˜…...")
        print("=" * 60)
        return self.feed_fetcher.fetch_all()

    def fetch_youtube_videos(self) -> Dict:
        """è·å– YouTube è§†é¢‘"""
        print("\n" + "=" * 60)
        print("ğŸ¬ æ­£åœ¨è·å– YouTube è§†é¢‘...")
        print("=" * 60)
        
        if self.youtube_fetcher:
            videos = self.youtube_fetcher.fetch_selected_leaders(self.selected_leaders)
            # Log usage
            self.usage_tracker.log_youtube_usage(
                self.youtube_fetcher.request_count,
                self.youtube_fetcher.total_quota_used
            )
            return {"type": "api", "data": videos}
        else:
            recommendations = self.youtube_no_api.get_recommendations(self.selected_leaders)
            return {"type": "recommendations", "data": recommendations}
    
    def fetch_github_data(self) -> Dict:
        """è·å– GitHub Issues å’Œ Trending"""
        print("\n" + "=" * 60)
        print("ğŸ™ æ­£åœ¨è·å– GitHub æ•°æ®...")
        print("=" * 60)
        
        issues = self.github_fetcher.fetch_all_issues(max_per_repo=5)
        issues = self.deduplicator.filter_new('github_issue', issues, lambda x: x.unique_id)
        
        trending = self.github_fetcher.fetch_trending(since='daily', max_results=10)
        trending = self.deduplicator.filter_new('github_trending', trending, lambda x: x.unique_id)
        
        return {"issues": issues, "trending": trending}
    
    def fetch_reddit_data(self) -> List[RedditPost]:
        """è·å– Reddit å¸–å­"""
        print("\n" + "=" * 60)
        print("ğŸ”´ æ­£åœ¨è·å– Reddit æ•°æ®...")
        print("=" * 60)
        
        posts = self.reddit_fetcher.fetch_all(max_per_subreddit=15)
        posts = self.deduplicator.filter_new('reddit', posts, lambda x: x.unique_id)
        return posts
    
    def fetch_hn_data(self) -> List[HNStory]:
        """è·å– Hacker News æ•°æ®"""
        print("\n" + "=" * 60)
        print("ğŸŸ  æ­£åœ¨è·å– Hacker News æ•°æ®...")
        print("=" * 60)
        
        stories = self.hn_fetcher.fetch_top_stories(min_score=50, max_results=15, filter_ai=True)
        stories = self.deduplicator.filter_new('hn', stories, lambda x: x.unique_id)
        return stories
    
    def generate_html_content(
        self, 
        briefing: str,
        papers: Dict[str, List[ArxivPaper]], 
        feeds: Dict[str, List[FeedItem]],
        youtube_data: Dict,
        github_data: Dict = None,
        reddit_posts: List = None,
        hn_stories: List = None
    ) -> str:
        """ç”Ÿæˆ HTML é‚®ä»¶å†…å®¹"""
        today = datetime.now().strftime('%Yå¹´%mæœˆ%dæ—¥')
        
        # Render markdown briefing to simple HTML paragraphs
        briefing_html = "".join([f"<p>{line}</p>" for line in briefing.split('\n') if line.strip()])
        
        # Stats HTML
        stats_html = f'''
        <div class="stats-box">
            <h4 style="margin:0 0 10px 0; color:#4a5568;">âš™ï¸ ç³»ç»Ÿè¿è¡Œç»Ÿè®¡</h4>
            <div style="display:flex; justify-content:space-between; font-size:12px; color:#718096;">
                <div>
                    <strong>ğŸ¤– AI æ¨¡å‹ (Qwen-72B)</strong><br>
                    è°ƒç”¨æ¬¡æ•°: {self.usage_tracker.llm_calls}<br>
                    Input Tokens: {self.usage_tracker.llm_input_tokens}<br>
                    Output Tokens: {self.usage_tracker.llm_output_tokens}
                </div>
                <div>
                    <strong>ğŸ¬ YouTube API</strong><br>
                    API è°ƒç”¨: {self.usage_tracker.youtube_api_calls}<br>
                    Quota æ¶ˆè€—: {self.usage_tracker.youtube_quota} units
                </div>
            </div>
        </div>
        '''

        html = f'''
<!DOCTYPE html>
<html>
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <style>
        body {{
            font-family: -apple-system, BlinkMacSystemFont, "Segoe UI", Roboto, "Helvetica Neue", Arial, sans-serif;
            line-height: 1.6;
            color: #333;
            max-width: 900px;
            margin: 0 auto;
            padding: 20px;
            background: #f5f7fa;
        }}
        .container {{
            background: white;
            border-radius: 16px;
            padding: 40px;
            box-shadow: 0 4px 20px rgba(0,0,0,0.08);
        }}
        .header {{
            text-align: center;
            padding: 30px 0;
            border-bottom: 3px solid #667eea;
            margin-bottom: 30px;
        }}
        .header h1 {{
            color: #2d3748;
            margin: 0;
            font-size: 32px;
            background: linear-gradient(135deg, #667eea 0%, #764ba2 100%);
            -webkit-background-clip: text;
            -webkit-text-fill-color: transparent;
        }}
        .date {{
            color: #718096;
            font-size: 16px;
            margin-top: 10px;
        }}
        .briefing-box {{
            background: linear-gradient(135deg, #e6fffa 0%, #b2f5ea 100%);
            border-radius: 12px;
            padding: 25px;
            margin-bottom: 40px;
            border-left: 5px solid #38b2ac;
        }}
        .briefing-title {{
            font-size: 20px;
            font-weight: bold;
            color: #234e52;
            margin-bottom: 15px;
            display: flex;
            align-items: center;
        }}
        .section {{
            margin: 40px 0;
        }}
        .section-header {{
            display: flex;
            align-items: center;
            margin-bottom: 25px;
            padding-bottom: 15px;
            border-bottom: 2px solid #e2e8f0;
        }}
        .section-icon {{
            font-size: 28px;
            margin-right: 15px;
        }}
        .section-title {{
            font-size: 24px;
            color: #2d3748;
            margin: 0;
        }}
        .section-subtitle {{
            font-size: 14px;
            color: #718096;
            margin-left: auto;
        }}
        /* Paper Cards */
        .paper-card {{
            background: #f7fafc;
            border-radius: 12px;
            padding: 20px;
            margin: 15px 0;
            border-left: 4px solid #667eea;
            transition: transform 0.2s, box-shadow 0.2s;
        }}
        .paper-card:hover {{
            transform: translateX(5px);
            box-shadow: 0 4px 12px rgba(102, 126, 234, 0.15);
        }}
        .paper-card.ad {{ border-left-color: #48bb78; }}
        .paper-title {{ font-size: 16px; font-weight: 600; margin-bottom: 8px; }}
        .paper-title a {{ color: #2d3748; text-decoration: none; }}
        .paper-title a:hover {{ color: #667eea; }}
        .paper-authors {{ font-size: 13px; color: #718096; margin-bottom: 10px; }}
        .paper-summary {{ font-size: 14px; color: #4a5568; line-height: 1.7; }}
        .paper-meta {{ display: flex; gap: 15px; margin-top: 12px; font-size: 12px; }}
        .paper-tag {{ display: inline-block; padding: 3px 10px; background: #edf2f7; border-radius: 12px; color: #4a5568; }}
        
        /* Feed Cards */
        .feed-list {{ list-style: none; padding: 0; }}
        .feed-item {{
            padding: 15px;
            border-bottom: 1px solid #edf2f7;
            display: flex;
            flex-direction: column;
        }}
        .feed-item:last-child {{ border-bottom: none; }}
        .feed-source {{ 
            font-size: 12px; 
            text-transform: uppercase; 
            color: #718096; 
            font-weight: bold;
            margin-bottom: 4px;
        }}
        .feed-title {{ font-size: 16px; font-weight: 600; margin-bottom: 5px; }}
        .feed-title a {{ color: #2b6cb0; text-decoration: none; }}
        .feed-title a:hover {{ text-decoration: underline; }}
        .feed-date {{ font-size: 12px; color: #a0aec0; }}

        .video-card {{
            background: linear-gradient(135deg, #1a1a2e 0%, #16213e 100%);
            border-radius: 12px;
            padding: 20px;
            margin: 15px 0;
            color: white;
        }}
        .video-title a {{ color: #ff6b6b; text-decoration: none; }}
        
        .footer {{
            margin-top: 50px;
            padding-top: 30px;
            border-top: 2px solid #e2e8f0;
            text-align: center;
            color: #718096;
            font-size: 14px;
        }}
        .stats-box {{
            background: #f1f5f9;
            border-radius: 8px;
            padding: 15px;
            margin-top: 30px;
            border: 1px solid #e2e8f0;
            text-align: left;
        }}
    </style>
</head>
<body>
    <div class="container">
        <div class="header">
            <h1>ğŸ”¬ AI ç ”ç©¶å‘¨æŠ¥</h1>
            <div class="date">{today}</div>
        </div>
        
        <!-- AI Daily Briefing -->
        <div class="briefing-box">
            <div class="briefing-title">â˜•ï¸ ä»Šæ—¥ AI ç®€æŠ¥</div>
            <div style="color: #2c7a7b; font-size: 15px; line-height: 1.8;">
                {briefing_html}
            </div>
        </div>
        
        <!-- arXiv Papers -->
        <div class="section">
            <div class="section-header">
                <span class="section-icon">ğŸ“š</span>
                <h2 class="section-title">æ ¸å¿ƒè®ºæ–‡ (ArXiv)</h2>
            </div>
            <h3 style="color: #4a5568; margin-top:20px;">ğŸ”¥ å¤§æ¨¡å‹å‰æ²¿</h3>
            {self._generate_papers_html(papers['llm'], 'llm')}
            
            <h3 style="color: #4a5568; margin-top:30px;">ğŸ“Š å¹¿å‘Šä¸æ¨èç®—æ³•</h3>
            {self._generate_papers_html(papers['advertising'], 'ad')}
        </div>
        
        <!-- RSS Feeds -->
        <div class="section">
            <div class="section-header">
                <span class="section-icon">ğŸ“¡</span>
                <h2 class="section-title">ä¸šç•ŒåŠ¨æ€</h2>
            </div>
            
            <div style="display: grid; grid-template-columns: 1fr 1fr; gap: 40px;">
                <div>
                    <h3 style="border-bottom: 2px solid #ed8936; padding-bottom: 10px; color: #c05621;">ğŸ¢ AI Labs æ›´æ–°</h3>
                    {self._generate_feeds_html(feeds.get('AI_Labs', []))}
                </div>
                <div>
                    <h3 style="border-bottom: 2px solid #48bb78; padding-bottom: 10px; color: #2f855a;">ğŸ’° é¡¶çº§é£æŠ•è§‚ç‚¹</h3>
                    {self._generate_feeds_html(feeds.get('VC_Trends', []))}
                </div>
            </div>
            
            <div style="margin-top: 40px;">
                 <h3 style="border-bottom: 2px solid #4299e1; padding-bottom: 10px; color: #2b6cb0;">ğŸ“° ç§‘æŠ€æ–°é—»ç²¾é€‰</h3>
                 {self._generate_feeds_html(feeds.get('Tech_News', []) + feeds.get('High_Quality_Filters', []))}
            </div>
        </div>
        
        <!-- YouTube Videos -->
        <div class="section">
            <div class="section-header">
                <span class="section-icon">ğŸ¬</span>
                <h2 class="section-title">ç§‘æŠ€é¢†è¢–è®¿è°ˆ</h2>
            </div>
            {self._generate_youtube_html(youtube_data)}
        </div>
        
        <!-- New Sources: GitHub, Reddit, HN -->
        {self._generate_community_section(github_data, reddit_posts, hn_stories)}
        
        {stats_html}
        
        <div class="footer">
            <p>ğŸ“… {today} | Daily Info System</p>
            <p>ğŸ’¡ Stay Hungry, Stay Foolish</p>
        </div>
    </div>
</body>
</html>
'''
        return html
    
    def _generate_papers_html(self, papers: List[ArxivPaper], paper_type: str) -> str:
        """ç”Ÿæˆè®ºæ–‡ HTML"""
        if not papers:
            return '<p style="color: #718096; font-style: italic;">ä»Šæ—¥æ— æ›´æ–°</p>'
        
        html_parts = []
        card_class = "paper-card" if paper_type == 'llm' else "paper-card ad"
        
        for paper in papers[:6]:  # Limit per section
            authors_str = ', '.join(paper.authors[:3])
            summary = paper.summary
            
            html_parts.append(f'''
            <div class="{card_class}">
                <div class="paper-title">
                    <a href="{paper.abs_url}" target="_blank">{paper.title}</a>
                </div>
                <div class="paper-authors">ğŸ‘¥ {authors_str}</div>
                <div class="paper-summary">{summary}</div>
                <div class="paper-meta">
                    <span class="paper-tag">ğŸ“… {paper.published[:10]}</span>
                    <a href="{paper.pdf_url}" class="paper-link" target="_blank">ğŸ“„ PDF</a>
                </div>
            </div>
            ''')
        return '\n'.join(html_parts)

    def _generate_feeds_html(self, items: List[FeedItem]) -> str:
        """ç”Ÿæˆ Feed åˆ—è¡¨ HTML"""
        if not items:
            return '<p style="color: #cbd5e0;">æš‚æ— åŠ¨æ€</p>'
        
        html = '<div class="feed-list">'
        for item in items[:8]: # Limit items per list
            html += f'''
            <div class="feed-item">
                <div class="feed-source">{item.source_name}</div>
                <div class="feed-title"><a href="{item.link}" target="_blank">{item.title}</a></div>
                <div class="feed-date">{item.published.strftime('%m-%d')}</div>
            </div>
            '''
        html += '</div>'
        return html

    def _generate_youtube_html(self, youtube_data: Dict) -> str:
        """ç”Ÿæˆ YouTube HTML (Simplified for brevity)"""
        if youtube_data["type"] == "api":
            return self._generate_youtube_api_html(youtube_data["data"])
        else:
            return self._generate_youtube_recommendations_html(youtube_data["data"])

    def _generate_youtube_api_html(self, videos_by_leader: Dict[str, List[YouTubeVideo]]) -> str:
        html_parts = []
        for leader, videos in videos_by_leader.items():
            if not videos: continue
            html_parts.append(f'<h4 style="margin: 20px 0 10px 0; color: #553c9a;">ğŸ‘¤ {leader}</h4>')
            for video in videos[:2]:
                html_parts.append(f'''
                <div class="video-card">
                    <div class="video-title"><a href="{video.watch_url}" target="_blank">ğŸ¥ {video.title}</a></div>
                    <div style="font-size: 12px; color: #a0aec0; margin-top:5px;">{video.description[:100]}...</div>
                </div>
                ''')
        return '\n'.join(html_parts) if html_parts else '<p>æš‚æ— æ–°è§†é¢‘</p>'

    def _generate_youtube_recommendations_html(self, recommendations: Dict) -> str:
        # Reusing the logic but simplified
        html = '<p>ç‚¹å‡»ä¸‹æ–¹é“¾æ¥æœç´¢æœ€æ–°è§†é¢‘ï¼š</p><div style="display:flex; gap:10px; flex-wrap:wrap;">'
        for leader, url in recommendations["search_links"].items():
            html += f'<a href="{url}" style="padding:5px 15px; background:#e53e3e; color:white; border-radius:15px; text-decoration:none;">{leader}</a>'
        html += '</div>'
        return html
    
    def _generate_community_section(self, github_data: Dict, reddit_posts: List, hn_stories: List) -> str:
        """ç”Ÿæˆå¼€æºç¤¾åŒºç‰ˆå— HTML (ä½¿ç”¨ AI ç­›é€‰)"""
        if not github_data and not reddit_posts and not hn_stories:
            return ''
        
        sections = []
        
        # GitHub Issues
        if github_data and github_data.get('issues'):
            print("  ğŸ¤– AI ç­›é€‰ GitHub Issues...")
            issues_dicts = [i.to_dict() for i in github_data['issues']]
            issues_html = self.ai_curator.curate(issues_dicts, "GitHub çƒ­é—¨ Issues (llama.cpp/vllm/transformers)", max_items=3)
            sections.append(f'''
            <div style="margin-bottom: 30px;">
                <h3 style="border-bottom: 2px solid #6366f1; padding-bottom: 10px; color: #4338ca;">ğŸ™ å¼€æºç¤¾åŒºåŠ¨æ€</h3>
                {issues_html}
            </div>
            ''')
        
        # GitHub Trending
        if github_data and github_data.get('trending'):
            print("  ğŸ¤– AI ç­›é€‰ GitHub Trending...")
            trending_dicts = [t.to_dict() for t in github_data['trending']]
            trending_html = self.ai_curator.curate(trending_dicts, "GitHub æ¯æ—¥è¶‹åŠ¿é¡¹ç›®", max_items=3)
            sections.append(f'''
            <div style="margin-bottom: 30px;">
                <h3 style="border-bottom: 2px solid #f59e0b; padding-bottom: 10px; color: #d97706;">ğŸ”¥ GitHub è¶‹åŠ¿é¡¹ç›®</h3>
                {trending_html}
            </div>
            ''')
        
        # Reddit
        if reddit_posts:
            print("  ğŸ¤– AI ç­›é€‰ Reddit å¸–å­...")
            reddit_dicts = [p.to_dict() for p in reddit_posts]
            reddit_html = self.ai_curator.curate(reddit_dicts, "Reddit r/LocalLLaMA çƒ­è®®", max_items=3)
            sections.append(f'''
            <div style="margin-bottom: 30px;">
                <h3 style="border-bottom: 2px solid #ef4444; padding-bottom: 10px; color: #dc2626;">ğŸ”´ Reddit çƒ­è®®</h3>
                {reddit_html}
            </div>
            ''')
        
        # Hacker News
        if hn_stories:
            print("  ğŸ¤– AI ç­›é€‰ Hacker News...")
            hn_dicts = [s.to_dict() for s in hn_stories]
            hn_html = self.ai_curator.curate(hn_dicts, "Hacker News AI ç›¸å…³çƒ­å¸–", max_items=3)
            sections.append(f'''
            <div style="margin-bottom: 30px;">
                <h3 style="border-bottom: 2px solid #f97316; padding-bottom: 10px; color: #ea580c;">ğŸŸ  Hacker News ç²¾é€‰</h3>
                {hn_html}
            </div>
            ''')
        
        # Merge AI curator usage into main tracker
        curator_usage = self.ai_curator.get_usage()
        self.usage_tracker.llm_calls += curator_usage['calls']
        self.usage_tracker.llm_input_tokens += curator_usage['input_tokens']
        self.usage_tracker.llm_output_tokens += curator_usage['output_tokens']
        
        if sections:
            return f'''
            <div class="section">
                <div class="section-header">
                    <span class="section-icon">ğŸŒ</span>
                    <h2 class="section-title">ç¤¾åŒºç²¾é€‰ (AI Curated)</h2>
                </div>
                {''.join(sections)}
            </div>
            '''
        return ''

    def send_email(self, to_email: str, subject: str, content: str, cc_emails: List[str] = []) -> bool:
        """å‘é€é‚®ä»¶ (æ”¯æŒ CC)"""
        smtp_server = os.getenv('SMTP_SERVER', 'smtp.qq.com')
        smtp_port = int(os.getenv('SMTP_PORT', '465'))
        from_email = os.getenv('FROM_EMAIL')
        email_password = os.getenv('EMAIL_PASSWORD')
        
        if not from_email or not email_password:
            print("âŒ é”™è¯¯: æœªè®¾ç½®é‚®ä»¶é…ç½®ç¯å¢ƒå˜é‡")
            return False
        
        try:
            message = MIMEMultipart('alternative')
            message['From'] = from_email
            message['To'] = to_email
            message['Subject'] = Header(subject, 'utf-8')
            
            if cc_emails:
                message['Cc'] = ', '.join(cc_emails)
            
            message.attach(MIMEText(content, 'html', 'utf-8'))
            
            print(f"\nğŸ“§ æ­£åœ¨è¿æ¥é‚®ä»¶æœåŠ¡å™¨...")
            if smtp_port == 465:
                import ssl
                context = ssl.create_default_context()
                server = smtplib.SMTP_SSL(smtp_server, smtp_port, context=context)
            else:
                server = smtplib.SMTP(smtp_server, smtp_port)
                server.starttls()
            
            server.login(from_email, email_password)
            
            # Recipients = To + Cc
            recipients = [to_email] + cc_emails
            
            print(f"ğŸš€ æ­£åœ¨å‘é€é‚®ä»¶ç»™: {recipients}...")
            server.sendmail(from_email, recipients, message.as_string())
            server.quit()
            
            print(f"âœ… é‚®ä»¶å‘é€æˆåŠŸï¼")
            return True
            
        except Exception as e:
            print(f"âŒ å‘é€é‚®ä»¶å¤±è´¥: {e}")
            return False
    

    def save_report_to_file(self, html_content: str):
        """ä¿å­˜æ—¥æŠ¥åˆ°æœ¬åœ° archives æ–‡ä»¶å¤¹"""
        try:
            # ç¡®ä¿ç›®å½•å­˜åœ¨
            archive_dir = os.path.join(os.path.dirname(os.path.dirname(__file__)), 'archives')
            if not os.path.exists(archive_dir):
                os.makedirs(archive_dir)
            
            # ç”Ÿæˆæ–‡ä»¶å
            date_str = datetime.now().strftime('%Y-%m-%d')
            filename = f"daily_report_{date_str}.html"
            filepath = os.path.join(archive_dir, filename)
            
            # å†™å…¥æ–‡ä»¶
            with open(filepath, 'w', encoding='utf-8') as f:
                f.write(html_content)
                
            print(f"âœ… æ—¥æŠ¥å·²ä¿å­˜åˆ°: {filepath}")
            return filepath
        except Exception as e:
            print(f"âŒ ä¿å­˜æ—¥æŠ¥æ–‡ä»¶å¤±è´¥: {e}")
            return None

    def run(self, to_email: Optional[str] = None):
        """è¿è¡Œä¸»æµç¨‹"""
        print("\n" + "=" * 60)
        print("ğŸš€ å¯åŠ¨ AI ç ”ç©¶ä¸èµ„è®¯æŠ“å–ä»»åŠ¡")
        print("=" * 60)
        
        if to_email is None:
            to_email = os.getenv('TO_EMAIL', 'huzhe06@gmail.com')
        
        # å®šä¹‰æŠ„é€äºº
        cc_list = ['zhuhuiqing13@163.com']
        extra_cc = os.getenv('CC_EMAIL')
        if extra_cc:
            cc_list.extend([email.strip() for email in extra_cc.split(',')])

        # 1. Fetch all data sources
        papers = self.fetch_arxiv_papers()
        feeds = self.fetch_feeds()
        youtube_data = self.fetch_youtube_videos()
        github_data = self.fetch_github_data()
        reddit_posts = self.fetch_reddit_data()
        hn_stories = self.fetch_hn_data()
        
        # 2. Generate Briefing
        briefing = self.generate_daily_briefing(papers, feeds, youtube_data)
        
        # 3. Generate Email Content
        print("\nğŸ¨ æ­£åœ¨ç”Ÿæˆ HTML é‚®ä»¶...")
        html_content = self.generate_html_content(
            briefing, papers, feeds, youtube_data,
            github_data=github_data,
            reddit_posts=reddit_posts,
            hn_stories=hn_stories
        )
        
        # 3.1 Save report and deduplication state
        self.save_report_to_file(html_content)
        self.deduplicator.save()
        
        # 4. Send Email
        today = datetime.now().strftime('%mæœˆ%dæ—¥')
        subject = f"æ—¥æŠ¥ | AI æ¯æ—¥ç®€æŠ¥ & ç ”ç©¶åŠ¨æ€ ({today})"
        
        success = self.send_email(to_email, subject, html_content, cc_emails=cc_list)
        return success


if __name__ == "__main__":
    sender = ResearchDigestSender()
    sender.run()

