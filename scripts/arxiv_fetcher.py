#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
arXiv è®ºæ–‡æŠ“å–æ¨¡å—
è·å–å¤§æ¨¡å‹ã€å¹¿å‘Šé¢†åŸŸçš„æœ€æ–°è®ºæ–‡ï¼Œå¹¶æ”¯æŒæŒ‰å¼•ç”¨æ•°æ’åº
"""

import urllib.request
import urllib.parse
import json
import xml.etree.ElementTree as ET
from datetime import datetime, timedelta
from dataclasses import dataclass
from typing import List, Optional, Dict


@dataclass
class ArxivPaper:
    """arXiv è®ºæ–‡æ•°æ®ç»“æ„"""
    title: str
    authors: List[str]
    summary: str
    arxiv_id: str
    published: str
    updated: str
    pdf_url: str
    abs_url: str
    categories: List[str]
    citation_count: int = 0  # å¼•ç”¨æ•° (æ¥è‡ª Semantic Scholar)
    
    def __str__(self):
        return f"[{self.citation_count} cites] {self.title} ({self.published[:10]})"


class ArxivFetcher:
    """arXiv è®ºæ–‡æŠ“å–å™¨"""
    
    BASE_URL = "http://export.arxiv.org/api/query"
    SEMANTIC_SCHOLAR_BATCH_URL = "https://api.semanticscholar.org/graph/v1/paper/batch"
    
    # å¤§æ¨¡å‹ç›¸å…³å…³é”®è¯
    LLM_KEYWORDS = [
        "large language model",
        "LLM",
        "GPT",
        "transformer",
        "BERT",
        "foundation model",
        "instruction tuning",
        "RLHF",
        "reinforcement learning from human feedback",
        "chain of thought",
        "prompt engineering",
        "in-context learning",
        "multimodal",
        "vision language model",
        "language model alignment",
        "neural machine translation",
        "text generation",
    ]
    
    # å¹¿å‘Šé¢†åŸŸå…³é”®è¯
    AD_KEYWORDS = [
        "computational advertising",
        "click-through rate prediction",
        "CTR prediction",
        "conversion rate prediction",
        "CVR prediction",
        "recommendation system",
        "ad ranking",
        "real-time bidding",
        "RTB",
        "programmatic advertising",
        "user behavior modeling",
        "display advertising",
        "sponsored search",
        "ad auction",
        "ad targeting",
    ]
    
    def __init__(self, max_results: int = 20):
        self.max_results = max_results
    
    def _build_query(self, keywords: List[str], category: str = "cs.CL") -> str:
        """æ„å»º arXiv API æŸ¥è¯¢å­—ç¬¦ä¸²"""
        # å°†å…³é”®è¯ç»„åˆæˆ OR æŸ¥è¯¢
        keyword_query = " OR ".join([f'ti:"{kw}" OR abs:"{kw}"' for kw in keywords[:5]])
        # é™åˆ¶åˆ†ç±»
        query = f"({keyword_query}) AND cat:{category}"
        return query
    
    def _parse_entry(self, entry: ET.Element, ns: dict) -> ArxivPaper:
        """è§£æå•ä¸ªè®ºæ–‡æ¡ç›®"""
        # æå– arXiv ID
        # id æ ¼å¼é€šå¸¸ä¸º http://arxiv.org/abs/2312.11805v1 æˆ– 2312.11805
        full_id = entry.find('atom:id', ns).text
        # å»æ‰ç‰ˆæœ¬å·ä»¥ä¾¿ Semantic Scholar è¯†åˆ« (å¦‚ 2312.11805v1 -> 2312.11805)
        arxiv_id = full_id.split('/abs/')[-1].split('v')[0] 
        
        # æå–æ ‡é¢˜ï¼ˆå»é™¤å¤šä½™ç©ºç™½ï¼‰
        title = entry.find('atom:title', ns).text.strip().replace('\n', ' ')
        
        # æå–ä½œè€…åˆ—è¡¨
        authors = [
            author.find('atom:name', ns).text 
            for author in entry.findall('atom:author', ns)
        ]
        
        # æå–æ‘˜è¦
        summary = entry.find('atom:summary', ns).text.strip().replace('\n', ' ')
        
        # æå–æ—¶é—´
        published = entry.find('atom:published', ns).text
        updated = entry.find('atom:updated', ns).text
        
        # æå–é“¾æ¥
        pdf_url = ""
        abs_url = ""
        for link in entry.findall('atom:link', ns):
            if link.get('title') == 'pdf':
                pdf_url = link.get('href')
            elif link.get('type') == 'text/html':
                abs_url = link.get('href')
        
        # å¦‚æœæ²¡æœ‰æ‰¾åˆ° abs_urlï¼Œä½¿ç”¨é»˜è®¤æ ¼å¼
        if not abs_url:
            abs_url = f"https://arxiv.org/abs/{arxiv_id}"
        if not pdf_url:
            pdf_url = f"https://arxiv.org/pdf/{arxiv_id}.pdf"
        
        # æå–åˆ†ç±»
        categories = [
            cat.get('term') 
            for cat in entry.findall('atom:category', ns)
        ]
        
        return ArxivPaper(
            title=title,
            authors=authors,
            summary=summary,
            arxiv_id=arxiv_id,
            published=published,
            updated=updated,
            pdf_url=pdf_url,
            abs_url=abs_url,
            categories=categories
        )
    
    def _fetch_citation_counts(self, papers: List[ArxivPaper]) -> Dict[str, int]:
        """ä» Semantic Scholar è·å–å¼•ç”¨æ•°"""
        if not papers:
            return {}
            
        print("ğŸ” æ­£åœ¨ä» Semantic Scholar è·å–å¼•ç”¨æ•°æ®...")
        
        # æ„é€ è¯·æ±‚ä½“ï¼ŒSemantic Scholar æ”¯æŒ ARXIV:å‰ç¼€
        paper_ids = [f"ARXIV:{p.arxiv_id}" for p in papers]
        
        citations_map = {}
        
        try:
            # æ‰¹é‡è¯·æ±‚ï¼Œå¦‚æœæ•°é‡å¾ˆå¤§åº”è¯¥åˆ†æ‰¹ï¼Œè¿™é‡Œå‡è®¾ max_results è¾ƒå° (<100)
            req = urllib.request.Request(
                f"{self.SEMANTIC_SCHOLAR_BATCH_URL}?fields=citationCount",
                data=json.dumps({"ids": paper_ids}).encode('utf-8'),
                headers={'Content-Type': 'application/json'}
            )
            
            with urllib.request.urlopen(req, timeout=15) as response:
                data = json.loads(response.read().decode('utf-8'))
                
            # Semantic Scholar è¿”å›çš„é¡ºåºå¯¹åº”è¯·æ±‚çš„é¡ºåº
            # å¦‚æœæ²¡æ‰¾åˆ°ï¼Œè¿”å› null
            for i, item in enumerate(data):
                if item and 'citationCount' in item:
                    # åŒ¹é…å›åŸå§‹ paper list çš„ arxiv_id
                    original_id = papers[i].arxiv_id
                    citations_map[original_id] = item['citationCount']
            
            print(f"  æˆåŠŸè·å– {len(citations_map)} ç¯‡è®ºæ–‡çš„å¼•ç”¨æ•°æ®")
                    
        except Exception as e:
            print(f"âš ï¸ è·å–å¼•ç”¨æ•°å¤±è´¥ (å¯èƒ½æ˜¯ API é™åˆ¶æˆ–ç½‘ç»œé—®é¢˜): {e}")
            
        return citations_map

    def fetch_papers(self, keywords: List[str], categories: List[str] = None) -> List[ArxivPaper]:
        """æŠ“å–è®ºæ–‡"""
        if categories is None:
            categories = ["cs.CL", "cs.LG", "cs.AI", "cs.IR"]
        
        all_papers = []
        seen_ids = set()
        
        # 1. ä» Arxiv è·å–è®ºæ–‡
        for category in categories:
            query = self._build_query(keywords, category)
            
            params = {
                'search_query': query,
                'start': 0,
                'max_results': self.max_results,
                'sortBy': 'submittedDate',
                'sortOrder': 'descending'
            }
            
            url = f"{self.BASE_URL}?{urllib.parse.urlencode(params)}"
            
            try:
                print(f"æ­£åœ¨è·å– {category} åˆ†ç±»çš„è®ºæ–‡...")
                with urllib.request.urlopen(url, timeout=30) as response:
                    data = response.read().decode('utf-8')
                
                # è§£æ XML
                root = ET.fromstring(data)
                ns = {'atom': 'http://www.w3.org/2005/Atom'}
                
                for entry in root.findall('atom:entry', ns):
                    paper = self._parse_entry(entry, ns)
                    if paper.arxiv_id not in seen_ids:
                        all_papers.append(paper)
                        seen_ids.add(paper.arxiv_id)
                
            except Exception as e:
                print(f"è·å– {category} åˆ†ç±»è®ºæ–‡æ—¶å‡ºé”™: {e}")
                continue
        
        # 2. è·å–å¼•ç”¨æ•°å¹¶æ’åº
        if all_papers:
            citations = self._fetch_citation_counts(all_papers)
            for paper in all_papers:
                paper.citation_count = citations.get(paper.arxiv_id, 0)
            
            # 3. æŒ‰å¼•ç”¨æ•°é™åºæ’åº
            # å¦‚æœå¼•ç”¨æ•°ç›¸åŒï¼Œä¿æŒåŸæœ‰é¡ºåºï¼ˆé€šå¸¸æ˜¯æ—¶é—´é¡ºåºï¼‰
            all_papers.sort(key=lambda x: x.citation_count, reverse=True)
            
        return all_papers
    
    def fetch_llm_papers(self) -> List[ArxivPaper]:
        """è·å–å¤§æ¨¡å‹ç›¸å…³è®ºæ–‡"""
        print("=" * 50)
        print("ğŸ“š æ­£åœ¨è·å–å¤§æ¨¡å‹é¢†åŸŸè®ºæ–‡...")
        print("=" * 50)
        return self.fetch_papers(self.LLM_KEYWORDS, ["cs.CL", "cs.LG", "cs.AI"])
    
    def fetch_ad_papers(self) -> List[ArxivPaper]:
        """è·å–å¹¿å‘Šé¢†åŸŸè®ºæ–‡"""
        print("=" * 50)
        print("ğŸ“Š æ­£åœ¨è·å–å¹¿å‘Šé¢†åŸŸè®ºæ–‡...")
        print("=" * 50)
        return self.fetch_papers(self.AD_KEYWORDS, ["cs.IR", "cs.LG", "cs.AI"])
    
    def fetch_all(self) -> dict:
        """è·å–æ‰€æœ‰é¢†åŸŸè®ºæ–‡"""
        return {
            'llm': self.fetch_llm_papers(),
            'advertising': self.fetch_ad_papers()
        }


def filter_recent_papers(papers: List[ArxivPaper], days: int = 7) -> List[ArxivPaper]:
    """è¿‡æ»¤æœ€è¿‘å‡ å¤©çš„è®ºæ–‡"""
    cutoff_date = datetime.now() - timedelta(days=days)
    recent_papers = []
    
    for paper in papers:
        try:
            # arXiv æ—¥æœŸæ ¼å¼: 2024-01-15T12:00:00Z
            pub_date = datetime.strptime(paper.published[:10], '%Y-%m-%d')
            if pub_date >= cutoff_date:
                recent_papers.append(paper)
        except:
            recent_papers.append(paper)
    
    return recent_papers


if __name__ == "__main__":
    fetcher = ArxivFetcher(max_results=10)
    
    # æµ‹è¯•è·å–è®ºæ–‡
    papers = fetcher.fetch_all()
    
    print(f"\næ‰¾åˆ° {len(papers['llm'])} ç¯‡å¤§æ¨¡å‹è®ºæ–‡")
    print(f"æ‰¾åˆ° {len(papers['advertising'])} ç¯‡å¹¿å‘Šé¢†åŸŸè®ºæ–‡")
    
    # æ‰“å°ç¤ºä¾‹ (å‰5ç¯‡å¼•ç”¨æœ€é«˜çš„)
    if papers['llm']:
        print("\nğŸ”¬ å¤§æ¨¡å‹è®ºæ–‡ç¤ºä¾‹ (æŒ‰å¼•ç”¨æ•°æ’åº):")
        for paper in papers['llm'][:5]:
            print(f"  ğŸ”¥ [{paper.citation_count} å¼•ç”¨] {paper.title}")
            print(f"     å‘å¸ƒ: {paper.published[:10]}")
            print(f"     é“¾æ¥: {paper.abs_url}")
