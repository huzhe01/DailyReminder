#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
arXiv è®ºæ–‡æŠ“å–æ¨¡å—
è·å–å¤§æ¨¡å‹ã€å¹¿å‘Šé¢†åŸŸçš„æœ€æ–°è®ºæ–‡
"""

import urllib.request
import urllib.parse
import xml.etree.ElementTree as ET
from datetime import datetime, timedelta
from dataclasses import dataclass
from typing import List, Optional


@dataclass
class ArxivPaper:
    """arXiv è®ºæ–‡æ•°æ®ç»“æ„"""
    title: str
    authors: List[str]
    summary: str
    arxiv_id: str
    published: str
    updated: str
    pdf_url: str
    abs_url: str
    categories: List[str]


class ArxivFetcher:
    """arXiv è®ºæ–‡æŠ“å–å™¨"""
    
    BASE_URL = "http://export.arxiv.org/api/query"
    
    # å¤§æ¨¡å‹ç›¸å…³å…³é”®è¯
    LLM_KEYWORDS = [
        "large language model",
        "LLM",
        "GPT",
        "transformer",
        "BERT",
        "foundation model",
        "instruction tuning",
        "RLHF",
        "reinforcement learning from human feedback",
        "chain of thought",
        "prompt engineering",
        "in-context learning",
        "multimodal",
        "vision language model",
        "language model alignment",
        "neural machine translation",
        "text generation",
    ]
    
    # å¹¿å‘Šé¢†åŸŸå…³é”®è¯
    AD_KEYWORDS = [
        "computational advertising",
        "click-through rate prediction",
        "CTR prediction",
        "conversion rate prediction",
        "CVR prediction",
        "recommendation system",
        "ad ranking",
        "real-time bidding",
        "RTB",
        "programmatic advertising",
        "user behavior modeling",
        "display advertising",
        "sponsored search",
        "ad auction",
        "ad targeting",
    ]
    
    def __init__(self, max_results: int = 20):
        self.max_results = max_results
    
    def _build_query(self, keywords: List[str], category: str = "cs.CL") -> str:
        """æ„å»º arXiv API æŸ¥è¯¢å­—ç¬¦ä¸²"""
        # å°†å…³é”®è¯ç»„åˆæˆ OR æŸ¥è¯¢
        keyword_query = " OR ".join([f'ti:"{kw}" OR abs:"{kw}"' for kw in keywords[:5]])
        # é™åˆ¶åˆ†ç±»
        query = f"({keyword_query}) AND cat:{category}"
        return query
    
    def _parse_entry(self, entry: ET.Element, ns: dict) -> ArxivPaper:
        """è§£æå•ä¸ªè®ºæ–‡æ¡ç›®"""
        # æå– arXiv ID
        arxiv_id = entry.find('atom:id', ns).text.split('/abs/')[-1]
        
        # æå–æ ‡é¢˜ï¼ˆå»é™¤å¤šä½™ç©ºç™½ï¼‰
        title = entry.find('atom:title', ns).text.strip().replace('\n', ' ')
        
        # æå–ä½œè€…åˆ—è¡¨
        authors = [
            author.find('atom:name', ns).text 
            for author in entry.findall('atom:author', ns)
        ]
        
        # æå–æ‘˜è¦
        summary = entry.find('atom:summary', ns).text.strip().replace('\n', ' ')
        
        # æå–æ—¶é—´
        published = entry.find('atom:published', ns).text
        updated = entry.find('atom:updated', ns).text
        
        # æå–é“¾æ¥
        pdf_url = ""
        abs_url = ""
        for link in entry.findall('atom:link', ns):
            if link.get('title') == 'pdf':
                pdf_url = link.get('href')
            elif link.get('type') == 'text/html':
                abs_url = link.get('href')
        
        # å¦‚æœæ²¡æœ‰æ‰¾åˆ° abs_urlï¼Œä½¿ç”¨é»˜è®¤æ ¼å¼
        if not abs_url:
            abs_url = f"https://arxiv.org/abs/{arxiv_id}"
        if not pdf_url:
            pdf_url = f"https://arxiv.org/pdf/{arxiv_id}.pdf"
        
        # æå–åˆ†ç±»
        categories = [
            cat.get('term') 
            for cat in entry.findall('atom:category', ns)
        ]
        
        return ArxivPaper(
            title=title,
            authors=authors,
            summary=summary,
            arxiv_id=arxiv_id,
            published=published,
            updated=updated,
            pdf_url=pdf_url,
            abs_url=abs_url,
            categories=categories
        )
    
    def fetch_papers(self, keywords: List[str], categories: List[str] = None) -> List[ArxivPaper]:
        """æŠ“å–è®ºæ–‡"""
        if categories is None:
            categories = ["cs.CL", "cs.LG", "cs.AI", "cs.IR"]
        
        all_papers = []
        seen_ids = set()
        
        for category in categories:
            query = self._build_query(keywords, category)
            
            params = {
                'search_query': query,
                'start': 0,
                'max_results': self.max_results,
                'sortBy': 'submittedDate',
                'sortOrder': 'descending'
            }
            
            url = f"{self.BASE_URL}?{urllib.parse.urlencode(params)}"
            
            try:
                print(f"æ­£åœ¨è·å– {category} åˆ†ç±»çš„è®ºæ–‡...")
                with urllib.request.urlopen(url, timeout=30) as response:
                    data = response.read().decode('utf-8')
                
                # è§£æ XML
                root = ET.fromstring(data)
                ns = {'atom': 'http://www.w3.org/2005/Atom'}
                
                for entry in root.findall('atom:entry', ns):
                    paper = self._parse_entry(entry, ns)
                    if paper.arxiv_id not in seen_ids:
                        all_papers.append(paper)
                        seen_ids.add(paper.arxiv_id)
                
            except Exception as e:
                print(f"è·å– {category} åˆ†ç±»è®ºæ–‡æ—¶å‡ºé”™: {e}")
                continue
        
        return all_papers
    
    def fetch_llm_papers(self) -> List[ArxivPaper]:
        """è·å–å¤§æ¨¡å‹ç›¸å…³è®ºæ–‡"""
        print("=" * 50)
        print("ğŸ“š æ­£åœ¨è·å–å¤§æ¨¡å‹é¢†åŸŸè®ºæ–‡...")
        print("=" * 50)
        return self.fetch_papers(self.LLM_KEYWORDS, ["cs.CL", "cs.LG", "cs.AI"])
    
    def fetch_ad_papers(self) -> List[ArxivPaper]:
        """è·å–å¹¿å‘Šé¢†åŸŸè®ºæ–‡"""
        print("=" * 50)
        print("ğŸ“Š æ­£åœ¨è·å–å¹¿å‘Šé¢†åŸŸè®ºæ–‡...")
        print("=" * 50)
        return self.fetch_papers(self.AD_KEYWORDS, ["cs.IR", "cs.LG", "cs.AI"])
    
    def fetch_all(self) -> dict:
        """è·å–æ‰€æœ‰é¢†åŸŸè®ºæ–‡"""
        return {
            'llm': self.fetch_llm_papers(),
            'advertising': self.fetch_ad_papers()
        }


def filter_recent_papers(papers: List[ArxivPaper], days: int = 7) -> List[ArxivPaper]:
    """è¿‡æ»¤æœ€è¿‘å‡ å¤©çš„è®ºæ–‡"""
    cutoff_date = datetime.now() - timedelta(days=days)
    recent_papers = []
    
    for paper in papers:
        try:
            # arXiv æ—¥æœŸæ ¼å¼: 2024-01-15T12:00:00Z
            pub_date = datetime.strptime(paper.published[:10], '%Y-%m-%d')
            if pub_date >= cutoff_date:
                recent_papers.append(paper)
        except:
            recent_papers.append(paper)
    
    return recent_papers


if __name__ == "__main__":
    fetcher = ArxivFetcher(max_results=10)
    
    # æµ‹è¯•è·å–è®ºæ–‡
    papers = fetcher.fetch_all()
    
    print(f"\næ‰¾åˆ° {len(papers['llm'])} ç¯‡å¤§æ¨¡å‹è®ºæ–‡")
    print(f"æ‰¾åˆ° {len(papers['advertising'])} ç¯‡å¹¿å‘Šé¢†åŸŸè®ºæ–‡")
    
    # æ‰“å°ç¤ºä¾‹
    if papers['llm']:
        print("\nğŸ”¬ å¤§æ¨¡å‹è®ºæ–‡ç¤ºä¾‹:")
        paper = papers['llm'][0]
        print(f"  æ ‡é¢˜: {paper.title}")
        print(f"  ä½œè€…: {', '.join(paper.authors[:3])}...")
        print(f"  é“¾æ¥: {paper.abs_url}")
